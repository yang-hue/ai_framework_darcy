{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import context, nn, Tensor, ops, jit, set_seed, data_sink\n",
    "from mindspore import dtype as mstype\n",
    "from sympy import Function, symbols, sin, cos, pi\n",
    "from mindflow.utils import load_yaml_config\n",
    "from mindflow.cell import FCSequential\n",
    "from mindflow.pde import PDEWithLoss, sympy_to_mindspore\n",
    "\n",
    "\n",
    "from src import create_training_dataset, create_test_dataset\n",
    "from src import calculate_l2_error\n",
    "\n",
    "set_seed(123456)\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set context for training: using graph mode for high performance training with GPU acceleration\n",
    "config = load_yaml_config(\"configs/darcy.yaml\")\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\", device_id=0)\n",
    "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataset\n",
    "geom_name = \"flow_region\"\n",
    "flow_train_dataset = create_training_dataset(config, geom_name)\n",
    "train_data = flow_train_dataset.create_dataset(\n",
    "    batch_size=config[\"data\"][\"train\"][\"batch_size\"], shuffle=True, drop_remainder=True\n",
    ")\n",
    "\n",
    "# create test dataset\n",
    "test_input, test_label = create_test_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network model\n",
    "model = FCSequential(in_channels=config[\"model\"][\"in_channels\"],\n",
    "                     out_channels=config[\"model\"][\"out_channels\"],\n",
    "                     neurons=config[\"model\"][\"neurons\"],\n",
    "                     layers=config[\"model\"][\"layers\"],\n",
    "                     residual=config[\"model\"][\"residual\"],\n",
    "                     act=config[\"model\"][\"activation\"],\n",
    "                     weight_init=config[\"model\"][\"weight_init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "params = model.trainable_params()\n",
    "optimizer = nn.Adam(params, learning_rate=config[\"optimizer\"][\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darcy2D(PDEWithLoss):\n",
    "    def __init__(self, model, loss_fn=nn.MSELoss()):\n",
    "        self.x, self.y = symbols(\"x y\")\n",
    "        self.u = Function(\"u\")(self.x, self.y)\n",
    "        self.v = Function(\"v\")(self.x, self.y)\n",
    "        self.p = Function(\"p\")(self.x, self.y)\n",
    "        self.in_vars = [self.x, self.y]\n",
    "        self.out_vars = [self.u, self.v, self.p]\n",
    "        self.loss_fn = loss_fn\n",
    "        self.bc_nodes = sympy_to_mindspore(self.bc(), self.in_vars, self.out_vars)\n",
    "        super(Darcy2D, self).__init__(model, self.in_vars, self.out_vars)\n",
    "\n",
    "    def force_function(self, x, y):\n",
    "        return 8 * pi**2 * sin(2 * pi * x) * cos(2 * pi * y)\n",
    "\n",
    "    def pde(self):\n",
    "        loss_1 = (\n",
    "            self.u.diff(self.x)\n",
    "            + self.v.diff(self.y)\n",
    "            - self.force_function(self.x, self.y)\n",
    "        )\n",
    "        loss_2 = self.u + self.p.diff(self.x)\n",
    "        loss_3 = self.v + self.p.diff(self.y)\n",
    "        return {\"loss_1\": loss_1, \"loss_2\": loss_2, \"loss_3\": loss_3}\n",
    "\n",
    "    def bc(self):\n",
    "        u_boundary = self.u - (-2 * pi * cos(2 * pi * self.x) * cos(2 * pi * self.y))\n",
    "\n",
    "        v_boundary = self.v - (2 * pi * sin(2 * pi * self.x) * sin(2 * pi * self.y))\n",
    "\n",
    "        p_boundary = self.p - (sin(2 * pi * self.x) * cos(2 * pi * self.y))\n",
    "\n",
    "        return {\n",
    "            \"u_boundary\": u_boundary,\n",
    "            \"v_boundary\": v_boundary,\n",
    "            \"p_boundary\": p_boundary,\n",
    "        }\n",
    "\n",
    "    def get_loss(self, pde_data, bc_data):\n",
    "        pde_res = ops.Concat(1)(self.parse_node(self.pde_nodes, inputs=pde_data))\n",
    "        pde_loss = self.loss_fn(\n",
    "            pde_res, Tensor(np.array([0.0]).astype(np.float32), mstype.float32)\n",
    "        )\n",
    "\n",
    "        bc_res = ops.Concat(1)(self.parse_node(self.bc_nodes, inputs=bc_data))\n",
    "        bc_loss = self.loss_fn(\n",
    "            bc_res, Tensor(np.array([0.0]).astype(np.float32), mstype.float32)\n",
    "        )\n",
    "\n",
    "        return pde_loss + bc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # define problem\n",
    "    problem = Darcy2D(model)\n",
    "\n",
    "    # prepare loss scaler\n",
    "    if use_ascend:\n",
    "        from mindspore.amp import DynamicLossScaler, all_finite, auto_mixed_precision\n",
    "        loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "        auto_mixed_precision(model, 'O3')\n",
    "    else:\n",
    "        loss_scaler = None\n",
    "\n",
    "    def forward_fn(pde_data, bc_data):\n",
    "        loss = problem.get_loss(pde_data, bc_data)\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.scale(loss)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=False)\n",
    "\n",
    "    @jit\n",
    "    def train_step(pde_data, bc_data):\n",
    "        loss, grads = grad_fn(pde_data, bc_data)\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.unscale(loss)\n",
    "            is_finite = all_finite(grads)\n",
    "            if is_finite:\n",
    "                grads = loss_scaler.unscale(grads)\n",
    "                loss = ops.depend(loss, optimizer(grads))\n",
    "            loss_scaler.adjust(is_finite)\n",
    "        else:\n",
    "            loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "    epochs = config[\"data\"][\"train\"][\"epochs\"]\n",
    "    steps_per_epochs = train_data.get_dataset_size()\n",
    "    sink_process = data_sink(train_step, train_data, sink_size=1)\n",
    "\n",
    "    for epoch in range(1, 1 + epochs):\n",
    "        local_time_beg = time.time()\n",
    "        model.set_train(True)\n",
    "        for _ in range(steps_per_epochs):\n",
    "            cur_loss = sink_process()\n",
    "        print(f\"epoch: {epoch} train loss: {cur_loss} epoch time: {(time.time() - local_time_beg) * 1000 :.3f} ms\")\n",
    "        model.set_train(False)\n",
    "        if epoch % config[\"summary\"][\"eval_interval_epochs\"] == 0:\n",
    "            calculate_l2_error(model, test_input, test_label, config[\"data\"][\"train\"][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_boundary: u(x, y) + 2*pi*cos(2*pi*x)*cos(2*pi*y)\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "v_boundary: v(x, y) - 2*pi*sin(2*pi*x)*sin(2*pi*y)\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "p_boundary: p(x, y) - sin(2*pi*x)*cos(2*pi*y)\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "loss_1: -8*pi**2*sin(2*pi*x)*cos(2*pi*y) + Derivative(u(x, y), x) + Derivative(v(x, y), y)\n",
      "    Item numbers of current derivative formula nodes: 3\n",
      "loss_2: u(x, y) + Derivative(p(x, y), x)\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "loss_3: v(x, y) + Derivative(p(x, y), y)\n",
      "    Item numbers of current derivative formula nodes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:32.521.663 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:32.522.684 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:32.524.137 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:32.526.768 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:32.527.716 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:32.529.084 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:35.536.086 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/2535299276.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:35.536.167 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/2535299276.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:35.579.788 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:35.582.977 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:35.583.540 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:35.586.315 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:35.587.992 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n",
      "[ERROR] CORE(212411,7fe54c554740,python):2024-06-11-22:10:35.590.242 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_212411/1475634962.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 523.6344 epoch time: 6663.886 ms\n",
      "epoch: 2 train loss: 521.4643 epoch time: 2568.312 ms\n",
      "epoch: 3 train loss: 522.4199 epoch time: 2585.766 ms\n",
      "epoch: 4 train loss: 530.79877 epoch time: 2535.153 ms\n",
      "epoch: 5 train loss: 526.6251 epoch time: 2560.666 ms\n",
      "epoch: 6 train loss: 531.1123 epoch time: 2515.944 ms\n",
      "epoch: 7 train loss: 522.90784 epoch time: 2708.786 ms\n",
      "epoch: 8 train loss: 529.28906 epoch time: 2966.518 ms\n",
      "epoch: 9 train loss: 527.5852 epoch time: 2728.937 ms\n",
      "epoch: 10 train loss: 523.2656 epoch time: 2703.769 ms\n",
      "epoch: 11 train loss: 522.587 epoch time: 2509.272 ms\n",
      "epoch: 12 train loss: 526.1701 epoch time: 2532.090 ms\n",
      "epoch: 13 train loss: 531.89575 epoch time: 2522.648 ms\n",
      "epoch: 14 train loss: 530.9858 epoch time: 2505.418 ms\n",
      "epoch: 15 train loss: 529.33984 epoch time: 2471.594 ms\n",
      "epoch: 16 train loss: 526.0478 epoch time: 2524.454 ms\n",
      "epoch: 17 train loss: 522.0132 epoch time: 2769.181 ms\n",
      "epoch: 18 train loss: 531.60345 epoch time: 2566.707 ms\n",
      "epoch: 19 train loss: 521.3289 epoch time: 2600.660 ms\n",
      "epoch: 20 train loss: 520.66156 epoch time: 2737.093 ms\n",
      "epoch: 21 train loss: 524.7386 epoch time: 2507.818 ms\n",
      "epoch: 22 train loss: 529.0003 epoch time: 2656.638 ms\n",
      "epoch: 23 train loss: 526.29315 epoch time: 2451.138 ms\n",
      "epoch: 24 train loss: 530.7194 epoch time: 2514.160 ms\n",
      "epoch: 25 train loss: 520.59186 epoch time: 2474.597 ms\n",
      "epoch: 26 train loss: 535.3198 epoch time: 2516.451 ms\n",
      "epoch: 27 train loss: 518.1212 epoch time: 2512.668 ms\n",
      "epoch: 28 train loss: 523.3887 epoch time: 2546.893 ms\n",
      "epoch: 29 train loss: 522.891 epoch time: 2538.153 ms\n",
      "epoch: 30 train loss: 525.62305 epoch time: 2784.573 ms\n",
      "epoch: 31 train loss: 532.9788 epoch time: 2527.946 ms\n",
      "epoch: 32 train loss: 528.0618 epoch time: 2600.354 ms\n",
      "epoch: 33 train loss: 517.25854 epoch time: 2780.397 ms\n",
      "epoch: 34 train loss: 529.5957 epoch time: 2812.759 ms\n",
      "epoch: 35 train loss: 528.21075 epoch time: 2626.678 ms\n",
      "epoch: 36 train loss: 522.19525 epoch time: 2839.718 ms\n",
      "epoch: 37 train loss: 531.3528 epoch time: 2802.368 ms\n",
      "epoch: 38 train loss: 526.7863 epoch time: 2494.387 ms\n",
      "epoch: 39 train loss: 535.108 epoch time: 2505.993 ms\n",
      "epoch: 40 train loss: 524.53076 epoch time: 2523.318 ms\n",
      "epoch: 41 train loss: 527.21875 epoch time: 2521.003 ms\n",
      "epoch: 42 train loss: 527.8494 epoch time: 2513.304 ms\n",
      "epoch: 43 train loss: 528.90845 epoch time: 2589.685 ms\n",
      "epoch: 44 train loss: 523.4331 epoch time: 2507.721 ms\n",
      "epoch: 45 train loss: 525.93835 epoch time: 2840.205 ms\n",
      "epoch: 46 train loss: 528.7295 epoch time: 2684.218 ms\n",
      "epoch: 47 train loss: 522.2067 epoch time: 2567.998 ms\n",
      "epoch: 48 train loss: 524.03534 epoch time: 2507.202 ms\n",
      "epoch: 49 train loss: 527.8329 epoch time: 2490.561 ms\n",
      "epoch: 50 train loss: 523.2407 epoch time: 2517.021 ms\n",
      "epoch: 51 train loss: 526.5762 epoch time: 2573.965 ms\n",
      "epoch: 52 train loss: 523.2939 epoch time: 2700.428 ms\n",
      "epoch: 53 train loss: 528.13293 epoch time: 2485.972 ms\n",
      "epoch: 54 train loss: 522.1686 epoch time: 2530.775 ms\n",
      "epoch: 55 train loss: 526.23444 epoch time: 2531.701 ms\n",
      "epoch: 56 train loss: 524.924 epoch time: 2522.913 ms\n",
      "epoch: 57 train loss: 520.3008 epoch time: 2660.310 ms\n",
      "epoch: 58 train loss: 526.242 epoch time: 2493.362 ms\n",
      "epoch: 59 train loss: 530.8241 epoch time: 2482.565 ms\n",
      "epoch: 60 train loss: 527.21313 epoch time: 2498.592 ms\n",
      "epoch: 61 train loss: 526.34644 epoch time: 2536.554 ms\n",
      "epoch: 62 train loss: 521.7544 epoch time: 2513.876 ms\n",
      "epoch: 63 train loss: 525.7912 epoch time: 2623.797 ms\n",
      "epoch: 64 train loss: 530.4776 epoch time: 2783.770 ms\n",
      "epoch: 65 train loss: 525.23615 epoch time: 2551.427 ms\n",
      "epoch: 66 train loss: 525.48645 epoch time: 2546.699 ms\n",
      "epoch: 67 train loss: 532.4117 epoch time: 2706.489 ms\n",
      "epoch: 68 train loss: 525.5294 epoch time: 2694.867 ms\n",
      "epoch: 69 train loss: 527.51776 epoch time: 2728.511 ms\n",
      "epoch: 70 train loss: 526.15656 epoch time: 2847.775 ms\n",
      "epoch: 71 train loss: 525.96173 epoch time: 2522.698 ms\n",
      "epoch: 72 train loss: 522.32666 epoch time: 2612.993 ms\n",
      "epoch: 73 train loss: 525.30383 epoch time: 2645.857 ms\n",
      "epoch: 74 train loss: 524.4821 epoch time: 2506.493 ms\n",
      "epoch: 75 train loss: 522.7961 epoch time: 2648.914 ms\n",
      "epoch: 76 train loss: 518.116 epoch time: 2855.154 ms\n",
      "epoch: 77 train loss: 525.7676 epoch time: 2657.074 ms\n",
      "epoch: 78 train loss: 524.9665 epoch time: 2571.965 ms\n",
      "epoch: 79 train loss: 531.76807 epoch time: 2569.334 ms\n",
      "epoch: 80 train loss: 522.0453 epoch time: 2491.470 ms\n",
      "epoch: 81 train loss: 527.38885 epoch time: 2511.576 ms\n",
      "epoch: 82 train loss: 523.22015 epoch time: 2668.570 ms\n",
      "epoch: 83 train loss: 527.91785 epoch time: 2944.311 ms\n",
      "epoch: 84 train loss: 516.86426 epoch time: 2804.405 ms\n",
      "epoch: 85 train loss: 527.89044 epoch time: 2872.568 ms\n",
      "epoch: 86 train loss: 527.1913 epoch time: 2621.881 ms\n",
      "epoch: 87 train loss: 531.9093 epoch time: 2505.147 ms\n",
      "epoch: 88 train loss: 520.673 epoch time: 2559.916 ms\n",
      "epoch: 89 train loss: 530.74866 epoch time: 2516.029 ms\n",
      "epoch: 90 train loss: 524.8139 epoch time: 2568.388 ms\n",
      "epoch: 91 train loss: 525.58777 epoch time: 2619.412 ms\n",
      "epoch: 92 train loss: 529.4966 epoch time: 2549.717 ms\n",
      "epoch: 93 train loss: 521.0791 epoch time: 2686.845 ms\n",
      "epoch: 94 train loss: 526.9291 epoch time: 2701.790 ms\n",
      "epoch: 95 train loss: 519.56793 epoch time: 2605.805 ms\n",
      "epoch: 96 train loss: 530.8599 epoch time: 2580.190 ms\n",
      "epoch: 97 train loss: 525.08905 epoch time: 2740.523 ms\n",
      "epoch: 98 train loss: 529.81354 epoch time: 2534.188 ms\n",
      "epoch: 99 train loss: 529.3031 epoch time: 2558.135 ms\n",
      "epoch: 100 train loss: 521.1788 epoch time: 2651.577 ms\n",
      "    predict total time: 160.60376167297363 ms\n",
      "    l2_error:  1.0009945578316775\n",
      "==================================================================================================\n",
      "epoch: 101 train loss: 523.9318 epoch time: 2598.248 ms\n",
      "epoch: 102 train loss: 528.84076 epoch time: 2483.202 ms\n",
      "epoch: 103 train loss: 526.6824 epoch time: 2630.857 ms\n",
      "epoch: 104 train loss: 529.7141 epoch time: 2500.396 ms\n",
      "epoch: 105 train loss: 525.0645 epoch time: 2739.277 ms\n",
      "epoch: 106 train loss: 531.01166 epoch time: 2477.825 ms\n",
      "epoch: 107 train loss: 528.426 epoch time: 2490.772 ms\n",
      "epoch: 108 train loss: 528.5448 epoch time: 2548.752 ms\n",
      "epoch: 109 train loss: 525.8233 epoch time: 2533.727 ms\n",
      "epoch: 110 train loss: 527.6526 epoch time: 2572.064 ms\n",
      "epoch: 111 train loss: 528.99786 epoch time: 2628.793 ms\n",
      "epoch: 112 train loss: 522.6572 epoch time: 2690.468 ms\n",
      "epoch: 113 train loss: 532.2373 epoch time: 2523.070 ms\n",
      "epoch: 114 train loss: 527.6487 epoch time: 2675.443 ms\n",
      "epoch: 115 train loss: 529.0882 epoch time: 2541.919 ms\n",
      "epoch: 116 train loss: 522.77124 epoch time: 2640.244 ms\n",
      "epoch: 117 train loss: 524.9487 epoch time: 2543.718 ms\n",
      "epoch: 118 train loss: 521.23517 epoch time: 2675.990 ms\n",
      "epoch: 119 train loss: 518.98553 epoch time: 2614.464 ms\n",
      "epoch: 120 train loss: 528.2943 epoch time: 2548.882 ms\n",
      "epoch: 121 train loss: 531.4608 epoch time: 2512.169 ms\n",
      "epoch: 122 train loss: 527.82416 epoch time: 2531.242 ms\n",
      "epoch: 123 train loss: 526.1625 epoch time: 2594.152 ms\n",
      "epoch: 124 train loss: 520.0198 epoch time: 2570.805 ms\n",
      "epoch: 125 train loss: 525.6228 epoch time: 2784.521 ms\n",
      "epoch: 126 train loss: 526.45374 epoch time: 2674.829 ms\n",
      "epoch: 127 train loss: 525.52527 epoch time: 2481.951 ms\n",
      "epoch: 128 train loss: 516.63074 epoch time: 2680.512 ms\n",
      "epoch: 129 train loss: 501.61725 epoch time: 2822.199 ms\n",
      "epoch: 130 train loss: 461.4488 epoch time: 2884.420 ms\n",
      "epoch: 131 train loss: 429.47073 epoch time: 2668.294 ms\n",
      "epoch: 132 train loss: 388.45718 epoch time: 2761.802 ms\n",
      "epoch: 133 train loss: 338.32437 epoch time: 2680.380 ms\n",
      "epoch: 134 train loss: 287.3803 epoch time: 2745.088 ms\n",
      "epoch: 135 train loss: 258.13586 epoch time: 2646.040 ms\n",
      "epoch: 136 train loss: 234.68123 epoch time: 2721.957 ms\n",
      "epoch: 137 train loss: 198.52367 epoch time: 2541.852 ms\n",
      "epoch: 138 train loss: 161.3175 epoch time: 2512.940 ms\n",
      "epoch: 139 train loss: 133.39896 epoch time: 2790.434 ms\n",
      "epoch: 140 train loss: 108.37995 epoch time: 2786.761 ms\n",
      "epoch: 141 train loss: 88.15983 epoch time: 3011.777 ms\n",
      "epoch: 142 train loss: 69.87464 epoch time: 2705.667 ms\n",
      "epoch: 143 train loss: 77.02253 epoch time: 2675.582 ms\n",
      "epoch: 144 train loss: 55.257496 epoch time: 2515.043 ms\n",
      "epoch: 145 train loss: 43.04012 epoch time: 2576.108 ms\n",
      "epoch: 146 train loss: 29.537163 epoch time: 2807.456 ms\n",
      "epoch: 147 train loss: 21.959894 epoch time: 2743.670 ms\n",
      "epoch: 148 train loss: 19.547497 epoch time: 2925.024 ms\n",
      "epoch: 149 train loss: 19.207699 epoch time: 2711.107 ms\n",
      "epoch: 150 train loss: 17.340952 epoch time: 2553.855 ms\n",
      "epoch: 151 train loss: 14.679083 epoch time: 2636.224 ms\n",
      "epoch: 152 train loss: 13.560569 epoch time: 2559.864 ms\n",
      "epoch: 153 train loss: 13.083542 epoch time: 2652.312 ms\n",
      "epoch: 154 train loss: 12.260811 epoch time: 2778.927 ms\n",
      "epoch: 155 train loss: 11.447847 epoch time: 2614.959 ms\n",
      "epoch: 156 train loss: 10.466578 epoch time: 2588.245 ms\n",
      "epoch: 157 train loss: 9.654066 epoch time: 2533.214 ms\n",
      "epoch: 158 train loss: 8.882835 epoch time: 2514.014 ms\n",
      "epoch: 159 train loss: 8.301617 epoch time: 2789.485 ms\n",
      "epoch: 160 train loss: 7.6688232 epoch time: 2842.442 ms\n",
      "epoch: 161 train loss: 7.407756 epoch time: 2762.678 ms\n",
      "epoch: 162 train loss: 6.91496 epoch time: 2875.825 ms\n",
      "epoch: 163 train loss: 6.5819054 epoch time: 2525.514 ms\n",
      "epoch: 164 train loss: 6.061131 epoch time: 2510.111 ms\n",
      "epoch: 165 train loss: 5.66518 epoch time: 2568.771 ms\n",
      "epoch: 166 train loss: 5.340986 epoch time: 2526.648 ms\n",
      "epoch: 167 train loss: 5.033725 epoch time: 2668.005 ms\n",
      "epoch: 168 train loss: 4.6967325 epoch time: 2664.239 ms\n",
      "epoch: 169 train loss: 4.4669485 epoch time: 2544.205 ms\n",
      "epoch: 170 train loss: 4.144511 epoch time: 2540.250 ms\n",
      "epoch: 171 train loss: 3.953977 epoch time: 2633.751 ms\n",
      "epoch: 172 train loss: 3.6743698 epoch time: 2532.444 ms\n",
      "epoch: 173 train loss: 3.5213108 epoch time: 2515.185 ms\n",
      "epoch: 174 train loss: 3.2607372 epoch time: 2522.627 ms\n",
      "epoch: 175 train loss: 3.1757932 epoch time: 2560.546 ms\n",
      "epoch: 176 train loss: 2.958919 epoch time: 2802.262 ms\n",
      "epoch: 177 train loss: 2.8431478 epoch time: 2819.160 ms\n",
      "epoch: 178 train loss: 2.6974857 epoch time: 2604.368 ms\n",
      "epoch: 179 train loss: 2.5576863 epoch time: 2534.198 ms\n",
      "epoch: 180 train loss: 2.454107 epoch time: 2582.829 ms\n",
      "epoch: 181 train loss: 2.2795014 epoch time: 2508.370 ms\n",
      "epoch: 182 train loss: 2.2220726 epoch time: 2718.166 ms\n",
      "epoch: 183 train loss: 2.082985 epoch time: 2586.630 ms\n",
      "epoch: 184 train loss: 2.0028603 epoch time: 2643.342 ms\n",
      "epoch: 185 train loss: 1.8648832 epoch time: 2507.715 ms\n",
      "epoch: 186 train loss: 1.7390895 epoch time: 2476.592 ms\n",
      "epoch: 187 train loss: 1.6619946 epoch time: 2792.018 ms\n",
      "epoch: 188 train loss: 1.5681353 epoch time: 2656.555 ms\n",
      "epoch: 189 train loss: 1.5306714 epoch time: 2652.564 ms\n",
      "epoch: 190 train loss: 1.7183244 epoch time: 2540.031 ms\n",
      "epoch: 191 train loss: 3.6651661 epoch time: 2848.334 ms\n",
      "epoch: 192 train loss: 4.045752 epoch time: 2747.050 ms\n",
      "epoch: 193 train loss: 1.9066842 epoch time: 2750.216 ms\n",
      "epoch: 194 train loss: 2.1368992 epoch time: 2748.041 ms\n",
      "epoch: 195 train loss: 2.1968517 epoch time: 2756.287 ms\n",
      "epoch: 196 train loss: 1.071428 epoch time: 2790.627 ms\n",
      "epoch: 197 train loss: 1.4979925 epoch time: 2690.621 ms\n",
      "epoch: 198 train loss: 1.178559 epoch time: 2912.762 ms\n",
      "epoch: 199 train loss: 0.9312802 epoch time: 2895.109 ms\n",
      "epoch: 200 train loss: 1.0357584 epoch time: 2921.126 ms\n",
      "    predict total time: 2.7594566345214844 ms\n",
      "    l2_error:  0.2272803341298445\n",
      "==================================================================================================\n",
      "epoch: 201 train loss: 0.8501322 epoch time: 2651.241 ms\n",
      "epoch: 202 train loss: 0.83307755 epoch time: 2895.616 ms\n",
      "epoch: 203 train loss: 0.76005137 epoch time: 2903.249 ms\n",
      "epoch: 204 train loss: 0.75182503 epoch time: 2868.431 ms\n",
      "epoch: 205 train loss: 0.7015586 epoch time: 2704.547 ms\n",
      "epoch: 206 train loss: 0.65925956 epoch time: 2929.476 ms\n",
      "epoch: 207 train loss: 0.6432118 epoch time: 2832.360 ms\n",
      "epoch: 208 train loss: 0.60861486 epoch time: 2720.041 ms\n",
      "epoch: 209 train loss: 0.59780824 epoch time: 2761.041 ms\n",
      "epoch: 210 train loss: 0.5663687 epoch time: 2535.386 ms\n",
      "epoch: 211 train loss: 0.54053706 epoch time: 2955.644 ms\n",
      "epoch: 212 train loss: 0.5112307 epoch time: 2817.687 ms\n",
      "epoch: 213 train loss: 0.50171196 epoch time: 3022.510 ms\n",
      "epoch: 214 train loss: 0.48445785 epoch time: 2958.594 ms\n",
      "epoch: 215 train loss: 0.47881693 epoch time: 2873.837 ms\n",
      "epoch: 216 train loss: 0.48273635 epoch time: 3036.992 ms\n",
      "epoch: 217 train loss: 0.4947301 epoch time: 2946.956 ms\n",
      "epoch: 218 train loss: 0.52456677 epoch time: 2947.483 ms\n",
      "epoch: 219 train loss: 0.7842046 epoch time: 2732.811 ms\n",
      "epoch: 220 train loss: 2.2657177 epoch time: 2607.892 ms\n",
      "epoch: 221 train loss: 2.898656 epoch time: 2605.929 ms\n",
      "epoch: 222 train loss: 0.4169653 epoch time: 2472.371 ms\n",
      "epoch: 223 train loss: 1.5112605 epoch time: 2502.868 ms\n",
      "epoch: 224 train loss: 0.39224935 epoch time: 2546.376 ms\n",
      "epoch: 225 train loss: 0.89332503 epoch time: 2646.052 ms\n",
      "epoch: 226 train loss: 0.3595929 epoch time: 2636.447 ms\n",
      "epoch: 227 train loss: 0.5718323 epoch time: 2447.550 ms\n",
      "epoch: 228 train loss: 0.3216251 epoch time: 2551.282 ms\n",
      "epoch: 229 train loss: 0.44334877 epoch time: 2487.402 ms\n",
      "epoch: 230 train loss: 0.28033382 epoch time: 2499.403 ms\n",
      "epoch: 231 train loss: 0.30351317 epoch time: 2594.671 ms\n",
      "epoch: 232 train loss: 0.27061933 epoch time: 2545.567 ms\n",
      "epoch: 233 train loss: 0.26778623 epoch time: 2536.400 ms\n",
      "epoch: 234 train loss: 0.2461631 epoch time: 2554.195 ms\n",
      "epoch: 235 train loss: 0.24335957 epoch time: 2801.082 ms\n",
      "epoch: 236 train loss: 0.2419387 epoch time: 2520.155 ms\n",
      "epoch: 237 train loss: 0.22634166 epoch time: 2522.401 ms\n",
      "epoch: 238 train loss: 0.24198708 epoch time: 2497.545 ms\n",
      "epoch: 239 train loss: 0.27872646 epoch time: 2574.130 ms\n",
      "epoch: 240 train loss: 0.27371433 epoch time: 2638.340 ms\n",
      "epoch: 241 train loss: 0.2373141 epoch time: 2837.654 ms\n",
      "epoch: 242 train loss: 0.23360619 epoch time: 2859.400 ms\n",
      "epoch: 243 train loss: 0.2371596 epoch time: 2839.284 ms\n",
      "epoch: 244 train loss: 0.26597798 epoch time: 2868.708 ms\n",
      "epoch: 245 train loss: 0.48707682 epoch time: 2790.510 ms\n",
      "epoch: 246 train loss: 1.2615478 epoch time: 2830.423 ms\n",
      "epoch: 247 train loss: 2.5038083 epoch time: 2849.584 ms\n",
      "epoch: 248 train loss: 0.5525335 epoch time: 2831.317 ms\n",
      "epoch: 249 train loss: 0.6835456 epoch time: 2796.830 ms\n",
      "epoch: 250 train loss: 0.77127826 epoch time: 2930.286 ms\n",
      "epoch: 251 train loss: 0.1999325 epoch time: 2976.130 ms\n",
      "epoch: 252 train loss: 0.50640714 epoch time: 2554.684 ms\n",
      "epoch: 253 train loss: 0.18319134 epoch time: 2747.743 ms\n",
      "epoch: 254 train loss: 0.22585718 epoch time: 2984.977 ms\n",
      "epoch: 255 train loss: 0.2311021 epoch time: 2860.172 ms\n",
      "epoch: 256 train loss: 0.14562668 epoch time: 2784.041 ms\n",
      "epoch: 257 train loss: 0.18075505 epoch time: 2750.870 ms\n",
      "epoch: 258 train loss: 0.17881282 epoch time: 2724.454 ms\n",
      "epoch: 259 train loss: 0.14605029 epoch time: 2817.724 ms\n",
      "epoch: 260 train loss: 0.139794 epoch time: 2754.869 ms\n",
      "epoch: 261 train loss: 0.14677098 epoch time: 2973.324 ms\n",
      "epoch: 262 train loss: 0.13135588 epoch time: 2772.965 ms\n",
      "epoch: 263 train loss: 0.131159 epoch time: 2776.029 ms\n",
      "epoch: 264 train loss: 0.13545944 epoch time: 3001.988 ms\n",
      "epoch: 265 train loss: 0.13989812 epoch time: 2699.117 ms\n",
      "epoch: 266 train loss: 0.17956103 epoch time: 2843.662 ms\n",
      "epoch: 267 train loss: 0.3949588 epoch time: 2785.200 ms\n",
      "epoch: 268 train loss: 1.5420612 epoch time: 3011.917 ms\n",
      "epoch: 269 train loss: 3.4023004 epoch time: 2910.610 ms\n",
      "epoch: 270 train loss: 0.33890188 epoch time: 2750.051 ms\n",
      "epoch: 271 train loss: 1.2516991 epoch time: 2700.360 ms\n",
      "epoch: 272 train loss: 0.3203175 epoch time: 2678.990 ms\n",
      "epoch: 273 train loss: 0.6731891 epoch time: 2826.534 ms\n",
      "epoch: 274 train loss: 0.14832054 epoch time: 2751.840 ms\n",
      "epoch: 275 train loss: 0.3683743 epoch time: 2697.843 ms\n",
      "epoch: 276 train loss: 0.11828327 epoch time: 3042.395 ms\n",
      "epoch: 277 train loss: 0.22964786 epoch time: 2857.289 ms\n",
      "epoch: 278 train loss: 0.11884104 epoch time: 2896.475 ms\n",
      "epoch: 279 train loss: 0.14341989 epoch time: 2744.087 ms\n",
      "epoch: 280 train loss: 0.12331473 epoch time: 2677.962 ms\n",
      "epoch: 281 train loss: 0.10338037 epoch time: 2638.899 ms\n",
      "epoch: 282 train loss: 0.119620495 epoch time: 2888.662 ms\n",
      "epoch: 283 train loss: 0.10033032 epoch time: 2544.487 ms\n",
      "epoch: 284 train loss: 0.11391725 epoch time: 2684.535 ms\n",
      "epoch: 285 train loss: 0.121262304 epoch time: 2729.515 ms\n",
      "epoch: 286 train loss: 0.10670707 epoch time: 2552.707 ms\n",
      "epoch: 287 train loss: 0.0975832 epoch time: 2529.794 ms\n",
      "epoch: 288 train loss: 0.08989426 epoch time: 2536.439 ms\n",
      "epoch: 289 train loss: 0.093785256 epoch time: 2535.537 ms\n",
      "epoch: 290 train loss: 0.09378911 epoch time: 2652.330 ms\n",
      "epoch: 291 train loss: 0.09838242 epoch time: 2817.170 ms\n",
      "epoch: 292 train loss: 0.111537606 epoch time: 2774.299 ms\n",
      "epoch: 293 train loss: 0.12240412 epoch time: 2775.069 ms\n",
      "epoch: 294 train loss: 0.1958241 epoch time: 2574.607 ms\n",
      "epoch: 295 train loss: 0.58843315 epoch time: 2488.801 ms\n",
      "epoch: 296 train loss: 1.9688638 epoch time: 2446.191 ms\n",
      "epoch: 297 train loss: 2.704505 epoch time: 2678.361 ms\n",
      "epoch: 298 train loss: 0.12102639 epoch time: 2882.287 ms\n",
      "epoch: 299 train loss: 1.2627546 epoch time: 2740.939 ms\n",
      "epoch: 300 train loss: 0.27490354 epoch time: 2691.177 ms\n",
      "    predict total time: 2.681732177734375 ms\n",
      "    l2_error:  0.04708666948576921\n",
      "==================================================================================================\n",
      "epoch: 301 train loss: 0.5126126 epoch time: 2714.905 ms\n",
      "epoch: 302 train loss: 0.1365015 epoch time: 3006.815 ms\n",
      "epoch: 303 train loss: 0.29086807 epoch time: 2708.599 ms\n",
      "epoch: 304 train loss: 0.13897407 epoch time: 2619.300 ms\n",
      "epoch: 305 train loss: 0.14130266 epoch time: 2799.681 ms\n",
      "epoch: 306 train loss: 0.13062151 epoch time: 2832.468 ms\n",
      "epoch: 307 train loss: 0.07931699 epoch time: 2892.667 ms\n",
      "epoch: 308 train loss: 0.12251972 epoch time: 2733.386 ms\n",
      "epoch: 309 train loss: 0.07370282 epoch time: 2759.082 ms\n",
      "epoch: 310 train loss: 0.07882424 epoch time: 2797.928 ms\n",
      "epoch: 311 train loss: 0.07793171 epoch time: 2909.692 ms\n",
      "epoch: 312 train loss: 0.06711483 epoch time: 2705.117 ms\n",
      "epoch: 313 train loss: 0.07174031 epoch time: 2882.072 ms\n",
      "epoch: 314 train loss: 0.07136972 epoch time: 2705.907 ms\n",
      "epoch: 315 train loss: 0.06568985 epoch time: 2702.946 ms\n",
      "epoch: 316 train loss: 0.07628532 epoch time: 2868.954 ms\n",
      "epoch: 317 train loss: 0.07230367 epoch time: 2880.507 ms\n",
      "epoch: 318 train loss: 0.07645751 epoch time: 2813.056 ms\n",
      "epoch: 319 train loss: 0.07939062 epoch time: 2712.036 ms\n",
      "epoch: 320 train loss: 0.08965042 epoch time: 2515.646 ms\n",
      "epoch: 321 train loss: 0.10674238 epoch time: 2705.677 ms\n",
      "epoch: 322 train loss: 0.1899085 epoch time: 2654.602 ms\n",
      "epoch: 323 train loss: 0.56411445 epoch time: 2639.523 ms\n",
      "epoch: 324 train loss: 2.0229945 epoch time: 2518.282 ms\n",
      "epoch: 325 train loss: 2.8756127 epoch time: 2517.149 ms\n",
      "epoch: 326 train loss: 0.077290304 epoch time: 2512.868 ms\n",
      "epoch: 327 train loss: 1.279642 epoch time: 2543.262 ms\n",
      "epoch: 328 train loss: 0.08664762 epoch time: 2496.907 ms\n",
      "epoch: 329 train loss: 0.6897895 epoch time: 2645.946 ms\n",
      "epoch: 330 train loss: 0.08140983 epoch time: 2832.605 ms\n",
      "epoch: 331 train loss: 0.3809029 epoch time: 2701.759 ms\n",
      "epoch: 332 train loss: 0.08009441 epoch time: 2556.162 ms\n",
      "epoch: 333 train loss: 0.19685525 epoch time: 2513.980 ms\n",
      "epoch: 334 train loss: 0.08372935 epoch time: 2560.781 ms\n",
      "epoch: 335 train loss: 0.0911926 epoch time: 2416.245 ms\n",
      "epoch: 336 train loss: 0.07504578 epoch time: 2651.486 ms\n",
      "epoch: 337 train loss: 0.0640671 epoch time: 2434.320 ms\n",
      "epoch: 338 train loss: 0.07946293 epoch time: 2542.332 ms\n",
      "epoch: 339 train loss: 0.059654005 epoch time: 2463.686 ms\n",
      "epoch: 340 train loss: 0.060089577 epoch time: 2821.480 ms\n",
      "epoch: 341 train loss: 0.06743022 epoch time: 2565.198 ms\n",
      "epoch: 342 train loss: 0.058757864 epoch time: 2848.702 ms\n",
      "epoch: 343 train loss: 0.056796398 epoch time: 2642.184 ms\n",
      "epoch: 344 train loss: 0.054913532 epoch time: 2734.930 ms\n",
      "epoch: 345 train loss: 0.051256668 epoch time: 2822.793 ms\n",
      "epoch: 346 train loss: 0.05162364 epoch time: 2889.827 ms\n",
      "epoch: 347 train loss: 0.053223144 epoch time: 2827.665 ms\n",
      "epoch: 348 train loss: 0.055066522 epoch time: 2782.812 ms\n",
      "epoch: 349 train loss: 0.062941454 epoch time: 2548.442 ms\n",
      "epoch: 350 train loss: 0.0657461 epoch time: 2534.950 ms\n",
      "epoch: 351 train loss: 0.088816404 epoch time: 2465.834 ms\n",
      "epoch: 352 train loss: 0.19240373 epoch time: 2494.935 ms\n",
      "epoch: 353 train loss: 0.7210317 epoch time: 2552.734 ms\n",
      "epoch: 354 train loss: 2.634341 epoch time: 2530.759 ms\n",
      "epoch: 355 train loss: 1.9230341 epoch time: 2588.155 ms\n",
      "epoch: 356 train loss: 0.1709624 epoch time: 2517.688 ms\n",
      "epoch: 357 train loss: 1.0507989 epoch time: 2542.197 ms\n",
      "epoch: 358 train loss: 0.05656501 epoch time: 2562.028 ms\n",
      "epoch: 359 train loss: 0.44581938 epoch time: 2563.534 ms\n",
      "epoch: 360 train loss: 0.05240853 epoch time: 2487.902 ms\n",
      "epoch: 361 train loss: 0.21182603 epoch time: 2520.891 ms\n",
      "epoch: 362 train loss: 0.06861117 epoch time: 2487.899 ms\n",
      "epoch: 363 train loss: 0.10609554 epoch time: 2499.979 ms\n",
      "epoch: 364 train loss: 0.067898735 epoch time: 2493.024 ms\n",
      "epoch: 365 train loss: 0.06068827 epoch time: 2754.071 ms\n",
      "epoch: 366 train loss: 0.07342379 epoch time: 2574.981 ms\n",
      "epoch: 367 train loss: 0.046692498 epoch time: 2706.640 ms\n",
      "epoch: 368 train loss: 0.052526064 epoch time: 2908.407 ms\n",
      "epoch: 369 train loss: 0.05141067 epoch time: 2637.054 ms\n",
      "epoch: 370 train loss: 0.04455222 epoch time: 2712.328 ms\n",
      "epoch: 371 train loss: 0.046338715 epoch time: 2899.482 ms\n",
      "epoch: 372 train loss: 0.052345585 epoch time: 2905.163 ms\n",
      "epoch: 373 train loss: 0.05130303 epoch time: 2844.905 ms\n",
      "epoch: 374 train loss: 0.059617247 epoch time: 2769.511 ms\n",
      "epoch: 375 train loss: 0.07277816 epoch time: 2622.236 ms\n",
      "epoch: 376 train loss: 0.08279066 epoch time: 2838.474 ms\n",
      "epoch: 377 train loss: 0.09392667 epoch time: 2954.150 ms\n",
      "epoch: 378 train loss: 0.1505312 epoch time: 2667.899 ms\n",
      "epoch: 379 train loss: 0.3164856 epoch time: 2642.395 ms\n",
      "epoch: 380 train loss: 0.66405386 epoch time: 2473.870 ms\n",
      "epoch: 381 train loss: 1.1601413 epoch time: 2551.956 ms\n",
      "epoch: 382 train loss: 1.0915023 epoch time: 2445.555 ms\n",
      "epoch: 383 train loss: 0.32721257 epoch time: 2606.624 ms\n",
      "epoch: 384 train loss: 0.057961617 epoch time: 2517.550 ms\n",
      "epoch: 385 train loss: 0.2962844 epoch time: 2493.963 ms\n",
      "epoch: 386 train loss: 0.3829593 epoch time: 2662.189 ms\n",
      "epoch: 387 train loss: 0.116108455 epoch time: 2739.230 ms\n",
      "epoch: 388 train loss: 0.056466624 epoch time: 2902.879 ms\n",
      "epoch: 389 train loss: 0.15724537 epoch time: 2748.203 ms\n",
      "epoch: 390 train loss: 0.20582344 epoch time: 2848.340 ms\n",
      "epoch: 391 train loss: 0.15102027 epoch time: 2799.563 ms\n",
      "epoch: 392 train loss: 0.10920727 epoch time: 2541.343 ms\n",
      "epoch: 393 train loss: 0.08451309 epoch time: 2496.072 ms\n",
      "epoch: 394 train loss: 0.093900084 epoch time: 2681.876 ms\n",
      "epoch: 395 train loss: 0.10570639 epoch time: 2785.361 ms\n",
      "epoch: 396 train loss: 0.14654931 epoch time: 2652.835 ms\n",
      "epoch: 397 train loss: 0.24133994 epoch time: 2693.644 ms\n",
      "epoch: 398 train loss: 0.49025512 epoch time: 2755.824 ms\n",
      "epoch: 399 train loss: 0.99867135 epoch time: 2678.631 ms\n",
      "epoch: 400 train loss: 1.1214528 epoch time: 2862.919 ms\n",
      "    predict total time: 2.622842788696289 ms\n",
      "    l2_error:  0.05001511852439833\n",
      "==================================================================================================\n",
      "epoch: 401 train loss: 0.36593533 epoch time: 2701.338 ms\n",
      "epoch: 402 train loss: 0.05144938 epoch time: 2887.372 ms\n",
      "epoch: 403 train loss: 0.32585412 epoch time: 2649.665 ms\n",
      "epoch: 404 train loss: 0.44341382 epoch time: 2747.492 ms\n",
      "epoch: 405 train loss: 0.2140432 epoch time: 2718.262 ms\n",
      "epoch: 406 train loss: 0.040438466 epoch time: 2610.726 ms\n",
      "epoch: 407 train loss: 0.14106932 epoch time: 2525.855 ms\n",
      "epoch: 408 train loss: 0.29661542 epoch time: 2481.330 ms\n",
      "epoch: 409 train loss: 0.2693836 epoch time: 2487.621 ms\n",
      "epoch: 410 train loss: 0.15696341 epoch time: 2560.962 ms\n",
      "epoch: 411 train loss: 0.07260995 epoch time: 2449.977 ms\n",
      "epoch: 412 train loss: 0.039583877 epoch time: 2481.123 ms\n",
      "epoch: 413 train loss: 0.034940384 epoch time: 2515.538 ms\n",
      "epoch: 414 train loss: 0.04348336 epoch time: 2723.242 ms\n",
      "epoch: 415 train loss: 0.066291966 epoch time: 2620.088 ms\n",
      "epoch: 416 train loss: 0.13480185 epoch time: 2582.916 ms\n",
      "epoch: 417 train loss: 0.37061742 epoch time: 2514.560 ms\n",
      "epoch: 418 train loss: 0.84505814 epoch time: 2477.308 ms\n",
      "epoch: 419 train loss: 1.3343639 epoch time: 2491.061 ms\n",
      "epoch: 420 train loss: 0.9106991 epoch time: 2603.721 ms\n",
      "epoch: 421 train loss: 0.116781704 epoch time: 2725.998 ms\n",
      "epoch: 422 train loss: 0.1283569 epoch time: 2741.245 ms\n",
      "epoch: 423 train loss: 0.2833669 epoch time: 2740.191 ms\n",
      "epoch: 424 train loss: 0.13277178 epoch time: 2587.620 ms\n",
      "epoch: 425 train loss: 0.03426625 epoch time: 2571.946 ms\n",
      "epoch: 426 train loss: 0.0736601 epoch time: 2529.482 ms\n",
      "epoch: 427 train loss: 0.10304219 epoch time: 2495.768 ms\n",
      "epoch: 428 train loss: 0.071348846 epoch time: 2835.682 ms\n",
      "epoch: 429 train loss: 0.044970192 epoch time: 2787.884 ms\n",
      "epoch: 430 train loss: 0.036232032 epoch time: 2742.191 ms\n",
      "epoch: 431 train loss: 0.030322766 epoch time: 3015.136 ms\n",
      "epoch: 432 train loss: 0.028540669 epoch time: 2829.657 ms\n",
      "epoch: 433 train loss: 0.03129837 epoch time: 2678.931 ms\n",
      "epoch: 434 train loss: 0.044848636 epoch time: 2696.597 ms\n",
      "epoch: 435 train loss: 0.08519308 epoch time: 2715.578 ms\n",
      "epoch: 436 train loss: 0.29869425 epoch time: 2802.641 ms\n",
      "epoch: 437 train loss: 1.2127651 epoch time: 2785.389 ms\n",
      "epoch: 438 train loss: 3.0548458 epoch time: 2824.730 ms\n",
      "epoch: 439 train loss: 0.7651304 epoch time: 2891.187 ms\n",
      "epoch: 440 train loss: 0.49100524 epoch time: 2625.561 ms\n",
      "epoch: 441 train loss: 0.6812758 epoch time: 2756.436 ms\n",
      "epoch: 442 train loss: 0.100970715 epoch time: 2770.926 ms\n",
      "epoch: 443 train loss: 0.44986522 epoch time: 2914.284 ms\n",
      "epoch: 444 train loss: 0.03825146 epoch time: 2837.106 ms\n",
      "epoch: 445 train loss: 0.20125084 epoch time: 2548.599 ms\n",
      "epoch: 446 train loss: 0.054341555 epoch time: 2589.889 ms\n",
      "epoch: 447 train loss: 0.07661842 epoch time: 2514.497 ms\n",
      "epoch: 448 train loss: 0.0785951 epoch time: 2586.348 ms\n",
      "epoch: 449 train loss: 0.027299441 epoch time: 2659.258 ms\n",
      "epoch: 450 train loss: 0.052790873 epoch time: 2483.953 ms\n",
      "epoch: 451 train loss: 0.058101065 epoch time: 2732.474 ms\n",
      "epoch: 452 train loss: 0.029933034 epoch time: 2802.411 ms\n",
      "epoch: 453 train loss: 0.033750746 epoch time: 2717.187 ms\n",
      "epoch: 454 train loss: 0.040971532 epoch time: 2890.336 ms\n",
      "epoch: 455 train loss: 0.034370836 epoch time: 2807.430 ms\n",
      "epoch: 456 train loss: 0.028387906 epoch time: 2769.403 ms\n",
      "epoch: 457 train loss: 0.029010195 epoch time: 2769.104 ms\n",
      "epoch: 458 train loss: 0.031456836 epoch time: 2754.995 ms\n",
      "epoch: 459 train loss: 0.03407214 epoch time: 2808.467 ms\n",
      "epoch: 460 train loss: 0.034204587 epoch time: 2703.204 ms\n",
      "epoch: 461 train loss: 0.043053437 epoch time: 2874.063 ms\n",
      "epoch: 462 train loss: 0.05767383 epoch time: 2906.271 ms\n",
      "epoch: 463 train loss: 0.07275111 epoch time: 2594.680 ms\n",
      "epoch: 464 train loss: 0.17027868 epoch time: 2712.733 ms\n",
      "epoch: 465 train loss: 0.5340886 epoch time: 2570.479 ms\n",
      "epoch: 466 train loss: 1.820981 epoch time: 2604.522 ms\n",
      "epoch: 467 train loss: 2.392573 epoch time: 2487.914 ms\n",
      "epoch: 468 train loss: 0.17239961 epoch time: 2626.871 ms\n",
      "epoch: 469 train loss: 0.7304933 epoch time: 2645.706 ms\n",
      "epoch: 470 train loss: 0.52560055 epoch time: 2541.949 ms\n",
      "epoch: 471 train loss: 0.108084306 epoch time: 2594.004 ms\n",
      "epoch: 472 train loss: 0.31351426 epoch time: 2590.888 ms\n",
      "epoch: 473 train loss: 0.031895325 epoch time: 2753.995 ms\n",
      "epoch: 474 train loss: 0.15298647 epoch time: 2845.008 ms\n",
      "epoch: 475 train loss: 0.0365294 epoch time: 2830.556 ms\n",
      "epoch: 476 train loss: 0.07621068 epoch time: 2747.292 ms\n",
      "epoch: 477 train loss: 0.06463202 epoch time: 2774.313 ms\n",
      "epoch: 478 train loss: 0.028489428 epoch time: 2696.967 ms\n",
      "epoch: 479 train loss: 0.052240033 epoch time: 2749.837 ms\n",
      "epoch: 480 train loss: 0.03665591 epoch time: 2793.899 ms\n",
      "epoch: 481 train loss: 0.02975312 epoch time: 2961.745 ms\n",
      "epoch: 482 train loss: 0.04201603 epoch time: 2644.309 ms\n",
      "epoch: 483 train loss: 0.036442082 epoch time: 2489.040 ms\n",
      "epoch: 484 train loss: 0.026527394 epoch time: 2980.357 ms\n",
      "epoch: 485 train loss: 0.026163723 epoch time: 2597.000 ms\n",
      "epoch: 486 train loss: 0.026669452 epoch time: 2830.079 ms\n",
      "epoch: 487 train loss: 0.02582724 epoch time: 2877.711 ms\n",
      "epoch: 488 train loss: 0.023781914 epoch time: 2831.842 ms\n",
      "epoch: 489 train loss: 0.027028931 epoch time: 2836.738 ms\n",
      "epoch: 490 train loss: 0.028730894 epoch time: 2601.289 ms\n",
      "epoch: 491 train loss: 0.037060425 epoch time: 2782.732 ms\n",
      "epoch: 492 train loss: 0.061833743 epoch time: 2788.695 ms\n",
      "epoch: 493 train loss: 0.1914591 epoch time: 2784.017 ms\n",
      "epoch: 494 train loss: 0.85044014 epoch time: 2776.088 ms\n",
      "epoch: 495 train loss: 2.2715774 epoch time: 2865.318 ms\n",
      "epoch: 496 train loss: 1.9603598 epoch time: 2710.513 ms\n",
      "epoch: 497 train loss: 0.08471635 epoch time: 2706.511 ms\n",
      "epoch: 498 train loss: 1.0683974 epoch time: 2929.851 ms\n",
      "epoch: 499 train loss: 0.058065984 epoch time: 2688.856 ms\n",
      "epoch: 500 train loss: 0.42015752 epoch time: 2941.870 ms\n",
      "    predict total time: 2.7735233306884766 ms\n",
      "    l2_error:  0.03666925064042334\n",
      "==================================================================================================\n",
      "epoch: 501 train loss: 0.07612733 epoch time: 2775.428 ms\n",
      "epoch: 502 train loss: 0.15308642 epoch time: 2672.117 ms\n",
      "epoch: 503 train loss: 0.0755029 epoch time: 2604.278 ms\n",
      "epoch: 504 train loss: 0.058375005 epoch time: 2936.395 ms\n",
      "epoch: 505 train loss: 0.083591916 epoch time: 2980.928 ms\n",
      "epoch: 506 train loss: 0.024022022 epoch time: 2879.689 ms\n",
      "epoch: 507 train loss: 0.04933 epoch time: 2789.378 ms\n",
      "epoch: 508 train loss: 0.03135899 epoch time: 2697.037 ms\n",
      "epoch: 509 train loss: 0.025366627 epoch time: 2909.296 ms\n",
      "epoch: 510 train loss: 0.030274449 epoch time: 2634.778 ms\n",
      "epoch: 511 train loss: 0.0238863 epoch time: 2838.906 ms\n",
      "epoch: 512 train loss: 0.02556206 epoch time: 2746.766 ms\n",
      "epoch: 513 train loss: 0.026208688 epoch time: 3055.521 ms\n",
      "epoch: 514 train loss: 0.027738826 epoch time: 2953.230 ms\n",
      "epoch: 515 train loss: 0.02637784 epoch time: 2497.125 ms\n",
      "epoch: 516 train loss: 0.02462557 epoch time: 2538.640 ms\n",
      "epoch: 517 train loss: 0.026940526 epoch time: 2496.201 ms\n",
      "epoch: 518 train loss: 0.025288874 epoch time: 2546.804 ms\n",
      "epoch: 519 train loss: 0.027856827 epoch time: 2519.786 ms\n",
      "epoch: 520 train loss: 0.03359374 epoch time: 2497.701 ms\n",
      "epoch: 521 train loss: 0.049183983 epoch time: 2516.689 ms\n",
      "epoch: 522 train loss: 0.085968435 epoch time: 2504.447 ms\n",
      "epoch: 523 train loss: 0.22780518 epoch time: 2817.616 ms\n",
      "epoch: 524 train loss: 1.0544815 epoch time: 2564.518 ms\n",
      "epoch: 525 train loss: 2.7934685 epoch time: 2519.808 ms\n",
      "epoch: 526 train loss: 1.194295 epoch time: 2596.056 ms\n",
      "epoch: 527 train loss: 0.18885158 epoch time: 2510.278 ms\n",
      "epoch: 528 train loss: 0.73548424 epoch time: 2504.044 ms\n",
      "epoch: 529 train loss: 0.039966155 epoch time: 2840.042 ms\n",
      "epoch: 530 train loss: 0.30146894 epoch time: 2833.588 ms\n",
      "epoch: 531 train loss: 0.08968858 epoch time: 2653.502 ms\n",
      "epoch: 532 train loss: 0.0828481 epoch time: 2615.669 ms\n",
      "epoch: 533 train loss: 0.087996624 epoch time: 2812.760 ms\n",
      "epoch: 534 train loss: 0.029240688 epoch time: 2509.181 ms\n",
      "epoch: 535 train loss: 0.05749181 epoch time: 2576.623 ms\n",
      "epoch: 536 train loss: 0.028135577 epoch time: 2478.566 ms\n",
      "epoch: 537 train loss: 0.035830908 epoch time: 2641.013 ms\n",
      "epoch: 538 train loss: 0.04329952 epoch time: 2415.662 ms\n",
      "epoch: 539 train loss: 0.026424417 epoch time: 2481.255 ms\n",
      "epoch: 540 train loss: 0.023326954 epoch time: 2488.463 ms\n",
      "epoch: 541 train loss: 0.029262258 epoch time: 2452.705 ms\n",
      "epoch: 542 train loss: 0.022487732 epoch time: 2519.302 ms\n",
      "epoch: 543 train loss: 0.022384234 epoch time: 2527.786 ms\n",
      "epoch: 544 train loss: 0.020693848 epoch time: 2654.171 ms\n",
      "epoch: 545 train loss: 0.023795083 epoch time: 2505.650 ms\n",
      "epoch: 546 train loss: 0.023388956 epoch time: 2493.810 ms\n",
      "epoch: 547 train loss: 0.02791684 epoch time: 2604.782 ms\n",
      "epoch: 548 train loss: 0.036462963 epoch time: 2518.158 ms\n",
      "epoch: 549 train loss: 0.060943794 epoch time: 2528.774 ms\n",
      "epoch: 550 train loss: 0.1512278 epoch time: 2679.457 ms\n",
      "epoch: 551 train loss: 0.4473261 epoch time: 2541.044 ms\n",
      "epoch: 552 train loss: 1.1880087 epoch time: 2485.463 ms\n",
      "epoch: 553 train loss: 1.6775856 epoch time: 2611.649 ms\n",
      "epoch: 554 train loss: 0.5777939 epoch time: 2897.221 ms\n",
      "epoch: 555 train loss: 0.11287793 epoch time: 2735.600 ms\n",
      "epoch: 556 train loss: 0.59117985 epoch time: 2824.888 ms\n",
      "epoch: 557 train loss: 0.25052714 epoch time: 2757.425 ms\n",
      "epoch: 558 train loss: 0.04599805 epoch time: 2644.737 ms\n",
      "epoch: 559 train loss: 0.2222382 epoch time: 2875.221 ms\n",
      "epoch: 560 train loss: 0.09946706 epoch time: 3018.377 ms\n",
      "epoch: 561 train loss: 0.025851019 epoch time: 3059.017 ms\n",
      "epoch: 562 train loss: 0.07827163 epoch time: 2591.318 ms\n",
      "epoch: 563 train loss: 0.047623612 epoch time: 2811.648 ms\n",
      "epoch: 564 train loss: 0.021970995 epoch time: 2816.166 ms\n",
      "epoch: 565 train loss: 0.02809752 epoch time: 2812.792 ms\n",
      "epoch: 566 train loss: 0.026831586 epoch time: 2811.593 ms\n",
      "epoch: 567 train loss: 0.019466678 epoch time: 2838.793 ms\n",
      "epoch: 568 train loss: 0.020763969 epoch time: 2773.909 ms\n",
      "epoch: 569 train loss: 0.01984178 epoch time: 2697.973 ms\n",
      "epoch: 570 train loss: 0.02346105 epoch time: 2799.747 ms\n",
      "epoch: 571 train loss: 0.034843694 epoch time: 2721.787 ms\n",
      "epoch: 572 train loss: 0.0686238 epoch time: 2655.834 ms\n",
      "epoch: 573 train loss: 0.1430936 epoch time: 2731.883 ms\n",
      "epoch: 574 train loss: 0.38150725 epoch time: 2935.831 ms\n",
      "epoch: 575 train loss: 1.2128867 epoch time: 2822.997 ms\n",
      "epoch: 576 train loss: 2.2242215 epoch time: 2812.861 ms\n",
      "epoch: 577 train loss: 0.40433347 epoch time: 2791.250 ms\n",
      "epoch: 578 train loss: 0.46366364 epoch time: 2752.604 ms\n",
      "epoch: 579 train loss: 0.63098574 epoch time: 2826.302 ms\n",
      "epoch: 580 train loss: 0.02563524 epoch time: 2683.895 ms\n",
      "epoch: 581 train loss: 0.2772055 epoch time: 2878.537 ms\n",
      "epoch: 582 train loss: 0.07525586 epoch time: 2665.264 ms\n",
      "epoch: 583 train loss: 0.07129822 epoch time: 2536.500 ms\n",
      "epoch: 584 train loss: 0.10053618 epoch time: 2831.692 ms\n",
      "epoch: 585 train loss: 0.021212475 epoch time: 2711.024 ms\n",
      "epoch: 586 train loss: 0.042781502 epoch time: 2688.963 ms\n",
      "epoch: 587 train loss: 0.03464732 epoch time: 2719.625 ms\n",
      "epoch: 588 train loss: 0.019403754 epoch time: 2677.852 ms\n",
      "epoch: 589 train loss: 0.022802213 epoch time: 2441.159 ms\n",
      "epoch: 590 train loss: 0.023201777 epoch time: 2450.157 ms\n",
      "epoch: 591 train loss: 0.021790255 epoch time: 2517.770 ms\n",
      "epoch: 592 train loss: 0.018417051 epoch time: 2505.491 ms\n",
      "epoch: 593 train loss: 0.020599615 epoch time: 2711.170 ms\n",
      "epoch: 594 train loss: 0.019715466 epoch time: 2660.413 ms\n",
      "epoch: 595 train loss: 0.02288376 epoch time: 2689.335 ms\n",
      "epoch: 596 train loss: 0.032367155 epoch time: 2753.357 ms\n",
      "epoch: 597 train loss: 0.0449644 epoch time: 2766.740 ms\n",
      "epoch: 598 train loss: 0.06373104 epoch time: 2851.782 ms\n",
      "epoch: 599 train loss: 0.1169171 epoch time: 2764.145 ms\n",
      "epoch: 600 train loss: 0.3222972 epoch time: 2934.307 ms\n",
      "    predict total time: 2.539396286010742 ms\n",
      "    l2_error:  0.03471159432052156\n",
      "==================================================================================================\n",
      "epoch: 601 train loss: 0.85667557 epoch time: 2759.037 ms\n",
      "epoch: 602 train loss: 1.8740516 epoch time: 2645.075 ms\n",
      "epoch: 603 train loss: 1.4085536 epoch time: 2678.581 ms\n",
      "epoch: 604 train loss: 0.08433209 epoch time: 2791.010 ms\n",
      "epoch: 605 train loss: 0.4608513 epoch time: 2669.765 ms\n",
      "epoch: 606 train loss: 0.18649106 epoch time: 2673.755 ms\n",
      "epoch: 607 train loss: 0.090424374 epoch time: 2740.171 ms\n",
      "epoch: 608 train loss: 0.1347129 epoch time: 2827.177 ms\n",
      "epoch: 609 train loss: 0.08839687 epoch time: 2836.582 ms\n",
      "epoch: 610 train loss: 0.020102948 epoch time: 2727.205 ms\n",
      "epoch: 611 train loss: 0.05522587 epoch time: 2727.434 ms\n",
      "epoch: 612 train loss: 0.05532538 epoch time: 2974.947 ms\n",
      "epoch: 613 train loss: 0.035019442 epoch time: 2888.444 ms\n",
      "epoch: 614 train loss: 0.019623794 epoch time: 2687.814 ms\n",
      "epoch: 615 train loss: 0.031333607 epoch time: 2946.414 ms\n",
      "epoch: 616 train loss: 0.025871953 epoch time: 2786.314 ms\n",
      "epoch: 617 train loss: 0.026476303 epoch time: 2585.459 ms\n",
      "epoch: 618 train loss: 0.023244742 epoch time: 2880.696 ms\n",
      "epoch: 619 train loss: 0.023163013 epoch time: 2678.633 ms\n",
      "epoch: 620 train loss: 0.028491471 epoch time: 2669.982 ms\n",
      "epoch: 621 train loss: 0.03605949 epoch time: 2706.104 ms\n",
      "epoch: 622 train loss: 0.07432633 epoch time: 2666.413 ms\n",
      "epoch: 623 train loss: 0.1823527 epoch time: 2797.626 ms\n",
      "epoch: 624 train loss: 0.47514445 epoch time: 2769.159 ms\n",
      "epoch: 625 train loss: 1.0528015 epoch time: 2928.497 ms\n",
      "epoch: 626 train loss: 1.3802441 epoch time: 2776.000 ms\n",
      "epoch: 627 train loss: 0.63941914 epoch time: 2931.308 ms\n",
      "epoch: 628 train loss: 0.028497726 epoch time: 2897.283 ms\n",
      "epoch: 629 train loss: 0.2988965 epoch time: 2671.721 ms\n",
      "epoch: 630 train loss: 0.19662692 epoch time: 2506.700 ms\n",
      "epoch: 631 train loss: 0.04224082 epoch time: 2608.134 ms\n",
      "epoch: 632 train loss: 0.05462285 epoch time: 2456.342 ms\n",
      "epoch: 633 train loss: 0.08664339 epoch time: 2568.330 ms\n",
      "epoch: 634 train loss: 0.05359288 epoch time: 2691.932 ms\n",
      "epoch: 635 train loss: 0.020380383 epoch time: 2758.030 ms\n",
      "epoch: 636 train loss: 0.026982969 epoch time: 2782.482 ms\n",
      "epoch: 637 train loss: 0.040986065 epoch time: 2689.906 ms\n",
      "epoch: 638 train loss: 0.04032626 epoch time: 2812.241 ms\n",
      "epoch: 639 train loss: 0.04505763 epoch time: 2647.547 ms\n",
      "epoch: 640 train loss: 0.051244892 epoch time: 2959.110 ms\n",
      "epoch: 641 train loss: 0.071893275 epoch time: 2921.029 ms\n",
      "epoch: 642 train loss: 0.1246435 epoch time: 2737.369 ms\n",
      "epoch: 643 train loss: 0.3168992 epoch time: 2784.313 ms\n",
      "epoch: 644 train loss: 0.6598222 epoch time: 2642.000 ms\n",
      "epoch: 645 train loss: 0.9151988 epoch time: 2778.494 ms\n",
      "epoch: 646 train loss: 0.9820852 epoch time: 2711.837 ms\n",
      "epoch: 647 train loss: 0.54911274 epoch time: 2852.148 ms\n",
      "epoch: 648 train loss: 0.027529903 epoch time: 2529.624 ms\n",
      "epoch: 649 train loss: 0.28745532 epoch time: 2633.346 ms\n",
      "epoch: 650 train loss: 0.15976816 epoch time: 2762.102 ms\n",
      "epoch: 651 train loss: 0.06927052 epoch time: 2813.262 ms\n",
      "epoch: 652 train loss: 0.022771094 epoch time: 2824.386 ms\n",
      "epoch: 653 train loss: 0.058090843 epoch time: 2729.729 ms\n",
      "epoch: 654 train loss: 0.052177656 epoch time: 2494.602 ms\n",
      "epoch: 655 train loss: 0.05027473 epoch time: 2828.725 ms\n",
      "epoch: 656 train loss: 0.04945644 epoch time: 2733.103 ms\n",
      "epoch: 657 train loss: 0.0493131 epoch time: 2472.658 ms\n",
      "epoch: 658 train loss: 0.05114086 epoch time: 2565.220 ms\n",
      "epoch: 659 train loss: 0.07669038 epoch time: 2833.360 ms\n",
      "epoch: 660 train loss: 0.16599312 epoch time: 2811.203 ms\n",
      "epoch: 661 train loss: 0.37373662 epoch time: 2873.981 ms\n",
      "epoch: 662 train loss: 0.86068577 epoch time: 2821.696 ms\n",
      "epoch: 663 train loss: 1.6850759 epoch time: 2766.333 ms\n",
      "epoch: 664 train loss: 0.8432793 epoch time: 2768.576 ms\n",
      "epoch: 665 train loss: 0.12683949 epoch time: 2516.992 ms\n",
      "epoch: 666 train loss: 0.72488564 epoch time: 2489.896 ms\n",
      "epoch: 667 train loss: 0.12365675 epoch time: 2531.864 ms\n",
      "epoch: 668 train loss: 0.19316179 epoch time: 2587.174 ms\n",
      "epoch: 669 train loss: 0.18336463 epoch time: 2537.531 ms\n",
      "epoch: 670 train loss: 0.03366578 epoch time: 2762.398 ms\n",
      "epoch: 671 train loss: 0.11144063 epoch time: 2684.519 ms\n",
      "epoch: 672 train loss: 0.09010063 epoch time: 2793.414 ms\n",
      "epoch: 673 train loss: 0.019504333 epoch time: 2849.195 ms\n",
      "epoch: 674 train loss: 0.05668402 epoch time: 2801.222 ms\n",
      "epoch: 675 train loss: 0.03268478 epoch time: 2738.213 ms\n",
      "epoch: 676 train loss: 0.023930104 epoch time: 3004.304 ms\n",
      "epoch: 677 train loss: 0.023137867 epoch time: 2759.911 ms\n",
      "epoch: 678 train loss: 0.025082482 epoch time: 2896.422 ms\n",
      "epoch: 679 train loss: 0.020108601 epoch time: 3042.287 ms\n",
      "epoch: 680 train loss: 0.016701892 epoch time: 2758.850 ms\n",
      "epoch: 681 train loss: 0.014900069 epoch time: 2869.074 ms\n",
      "epoch: 682 train loss: 0.015034212 epoch time: 2820.390 ms\n",
      "epoch: 683 train loss: 0.01709712 epoch time: 2703.025 ms\n",
      "epoch: 684 train loss: 0.022549413 epoch time: 2599.557 ms\n",
      "epoch: 685 train loss: 0.0252341 epoch time: 2746.542 ms\n",
      "epoch: 686 train loss: 0.041920822 epoch time: 2926.166 ms\n",
      "epoch: 687 train loss: 0.10857128 epoch time: 2860.262 ms\n",
      "epoch: 688 train loss: 0.46161136 epoch time: 2793.138 ms\n",
      "epoch: 689 train loss: 1.9162588 epoch time: 2638.036 ms\n",
      "epoch: 690 train loss: 1.8580273 epoch time: 3002.008 ms\n",
      "epoch: 691 train loss: 0.5204656 epoch time: 2942.948 ms\n",
      "epoch: 692 train loss: 0.642866 epoch time: 2645.766 ms\n",
      "epoch: 693 train loss: 0.4391909 epoch time: 2940.593 ms\n",
      "epoch: 694 train loss: 0.11732533 epoch time: 2692.960 ms\n",
      "epoch: 695 train loss: 0.31753102 epoch time: 2835.654 ms\n",
      "epoch: 696 train loss: 0.018569048 epoch time: 2967.006 ms\n",
      "epoch: 697 train loss: 0.117515005 epoch time: 2768.103 ms\n",
      "epoch: 698 train loss: 0.05417508 epoch time: 2833.211 ms\n",
      "epoch: 699 train loss: 0.024885662 epoch time: 2875.089 ms\n",
      "epoch: 700 train loss: 0.043291617 epoch time: 2740.372 ms\n",
      "    predict total time: 2.5382041931152344 ms\n",
      "    l2_error:  0.01984831987533965\n",
      "==================================================================================================\n",
      "epoch: 701 train loss: 0.022297747 epoch time: 2625.445 ms\n",
      "epoch: 702 train loss: 0.021107016 epoch time: 2538.630 ms\n",
      "epoch: 703 train loss: 0.02021447 epoch time: 2593.460 ms\n",
      "epoch: 704 train loss: 0.022569137 epoch time: 2569.742 ms\n",
      "epoch: 705 train loss: 0.020846495 epoch time: 2783.805 ms\n",
      "epoch: 706 train loss: 0.017964471 epoch time: 2900.816 ms\n",
      "epoch: 707 train loss: 0.01589453 epoch time: 2851.838 ms\n",
      "epoch: 708 train loss: 0.014038446 epoch time: 2885.535 ms\n",
      "epoch: 709 train loss: 0.01605573 epoch time: 2590.484 ms\n",
      "epoch: 710 train loss: 0.017332494 epoch time: 2526.701 ms\n",
      "epoch: 711 train loss: 0.015548215 epoch time: 2505.519 ms\n",
      "epoch: 712 train loss: 0.022449635 epoch time: 2559.932 ms\n",
      "epoch: 713 train loss: 0.0414518 epoch time: 2601.753 ms\n",
      "epoch: 714 train loss: 0.12007132 epoch time: 2495.378 ms\n",
      "epoch: 715 train loss: 0.3950641 epoch time: 2529.787 ms\n",
      "epoch: 716 train loss: 1.0469031 epoch time: 2486.446 ms\n",
      "epoch: 717 train loss: 1.789159 epoch time: 2485.596 ms\n",
      "epoch: 718 train loss: 0.7790903 epoch time: 2537.486 ms\n",
      "epoch: 719 train loss: 0.118266605 epoch time: 2517.284 ms\n",
      "epoch: 720 train loss: 0.4530189 epoch time: 2509.784 ms\n",
      "epoch: 721 train loss: 0.22112449 epoch time: 2447.141 ms\n",
      "epoch: 722 train loss: 0.03490497 epoch time: 2501.259 ms\n",
      "epoch: 723 train loss: 0.15738818 epoch time: 2509.275 ms\n",
      "epoch: 724 train loss: 0.08127724 epoch time: 2514.367 ms\n",
      "epoch: 725 train loss: 0.032339104 epoch time: 2900.530 ms\n",
      "epoch: 726 train loss: 0.050943695 epoch time: 2726.549 ms\n",
      "epoch: 727 train loss: 0.06862743 epoch time: 2672.324 ms\n",
      "epoch: 728 train loss: 0.025589135 epoch time: 2841.610 ms\n",
      "epoch: 729 train loss: 0.021455768 epoch time: 2844.108 ms\n",
      "epoch: 730 train loss: 0.03511321 epoch time: 2788.614 ms\n",
      "epoch: 731 train loss: 0.027411137 epoch time: 2781.812 ms\n",
      "epoch: 732 train loss: 0.027828347 epoch time: 2814.419 ms\n",
      "epoch: 733 train loss: 0.015048602 epoch time: 2747.445 ms\n",
      "epoch: 734 train loss: 0.014340352 epoch time: 2860.541 ms\n",
      "epoch: 735 train loss: 0.020344354 epoch time: 2596.682 ms\n",
      "epoch: 736 train loss: 0.017255533 epoch time: 2750.745 ms\n",
      "epoch: 737 train loss: 0.014893193 epoch time: 2808.039 ms\n",
      "epoch: 738 train loss: 0.021160964 epoch time: 2729.875 ms\n",
      "epoch: 739 train loss: 0.04339897 epoch time: 2609.454 ms\n",
      "epoch: 740 train loss: 0.15798418 epoch time: 2495.816 ms\n",
      "epoch: 741 train loss: 0.6927779 epoch time: 2541.587 ms\n",
      "epoch: 742 train loss: 2.4612255 epoch time: 2629.606 ms\n",
      "epoch: 743 train loss: 2.3767247 epoch time: 2834.924 ms\n",
      "epoch: 744 train loss: 0.12526368 epoch time: 2708.795 ms\n",
      "epoch: 745 train loss: 1.2849247 epoch time: 2823.022 ms\n",
      "epoch: 746 train loss: 0.033994365 epoch time: 2494.632 ms\n",
      "epoch: 747 train loss: 0.6548678 epoch time: 2503.103 ms\n",
      "epoch: 748 train loss: 0.06778008 epoch time: 2568.462 ms\n",
      "epoch: 749 train loss: 0.2922346 epoch time: 2816.578 ms\n",
      "epoch: 750 train loss: 0.08312491 epoch time: 2560.973 ms\n",
      "epoch: 751 train loss: 0.13424131 epoch time: 2895.710 ms\n",
      "epoch: 752 train loss: 0.057669874 epoch time: 2792.704 ms\n",
      "epoch: 753 train loss: 0.04990229 epoch time: 2817.741 ms\n",
      "epoch: 754 train loss: 0.034066997 epoch time: 2680.809 ms\n",
      "epoch: 755 train loss: 0.031695947 epoch time: 2668.829 ms\n",
      "epoch: 756 train loss: 0.018444827 epoch time: 2709.064 ms\n",
      "epoch: 757 train loss: 0.023958046 epoch time: 2603.707 ms\n",
      "epoch: 758 train loss: 0.01510229 epoch time: 2569.752 ms\n",
      "epoch: 759 train loss: 0.02118564 epoch time: 2483.995 ms\n",
      "epoch: 760 train loss: 0.0135330595 epoch time: 2454.704 ms\n",
      "epoch: 761 train loss: 0.018608406 epoch time: 2489.400 ms\n",
      "epoch: 762 train loss: 0.017906949 epoch time: 2522.550 ms\n",
      "epoch: 763 train loss: 0.019615537 epoch time: 2783.372 ms\n",
      "epoch: 764 train loss: 0.019905593 epoch time: 2922.699 ms\n",
      "epoch: 765 train loss: 0.016473973 epoch time: 2775.307 ms\n",
      "epoch: 766 train loss: 0.017254129 epoch time: 2745.915 ms\n",
      "epoch: 767 train loss: 0.017571267 epoch time: 2913.341 ms\n",
      "epoch: 768 train loss: 0.01875746 epoch time: 2792.548 ms\n",
      "epoch: 769 train loss: 0.033347793 epoch time: 2702.554 ms\n",
      "epoch: 770 train loss: 0.086325586 epoch time: 2827.816 ms\n",
      "epoch: 771 train loss: 0.31743586 epoch time: 2727.864 ms\n",
      "epoch: 772 train loss: 1.0394422 epoch time: 2528.402 ms\n",
      "epoch: 773 train loss: 0.8739127 epoch time: 2524.409 ms\n",
      "epoch: 774 train loss: 0.578479 epoch time: 2758.787 ms\n",
      "epoch: 775 train loss: 0.032808498 epoch time: 2583.984 ms\n",
      "epoch: 776 train loss: 0.288373 epoch time: 2560.102 ms\n",
      "epoch: 777 train loss: 0.11768926 epoch time: 2499.563 ms\n",
      "epoch: 778 train loss: 0.053532757 epoch time: 2526.077 ms\n",
      "epoch: 779 train loss: 0.057643086 epoch time: 2632.053 ms\n",
      "epoch: 780 train loss: 0.019290363 epoch time: 2511.695 ms\n",
      "epoch: 781 train loss: 0.045118928 epoch time: 2735.984 ms\n",
      "epoch: 782 train loss: 0.03825629 epoch time: 2486.705 ms\n",
      "epoch: 783 train loss: 0.044198036 epoch time: 2529.357 ms\n",
      "epoch: 784 train loss: 0.053117797 epoch time: 2445.717 ms\n",
      "epoch: 785 train loss: 0.05976491 epoch time: 2493.364 ms\n",
      "epoch: 786 train loss: 0.09125857 epoch time: 2530.571 ms\n",
      "epoch: 787 train loss: 0.18599775 epoch time: 2528.233 ms\n",
      "epoch: 788 train loss: 0.46567857 epoch time: 2477.250 ms\n",
      "epoch: 789 train loss: 1.0165501 epoch time: 2462.498 ms\n",
      "epoch: 790 train loss: 1.3866825 epoch time: 2525.787 ms\n",
      "epoch: 791 train loss: 0.4699763 epoch time: 2489.701 ms\n",
      "epoch: 792 train loss: 0.062318057 epoch time: 2839.210 ms\n",
      "epoch: 793 train loss: 0.43705806 epoch time: 2706.934 ms\n",
      "epoch: 794 train loss: 0.15982628 epoch time: 2471.464 ms\n",
      "epoch: 795 train loss: 0.0317424 epoch time: 2531.067 ms\n",
      "epoch: 796 train loss: 0.11515652 epoch time: 2473.139 ms\n",
      "epoch: 797 train loss: 0.051696327 epoch time: 2556.577 ms\n",
      "epoch: 798 train loss: 0.014070478 epoch time: 2479.404 ms\n",
      "epoch: 799 train loss: 0.037414193 epoch time: 2521.168 ms\n",
      "epoch: 800 train loss: 0.04317809 epoch time: 2531.718 ms\n",
      "    predict total time: 2.5434494018554688 ms\n",
      "    l2_error:  0.01833958302860856\n",
      "==================================================================================================\n",
      "epoch: 801 train loss: 0.028676331 epoch time: 2527.085 ms\n",
      "epoch: 802 train loss: 0.014792139 epoch time: 2484.532 ms\n",
      "epoch: 803 train loss: 0.012805542 epoch time: 2561.831 ms\n",
      "epoch: 804 train loss: 0.01641931 epoch time: 2677.458 ms\n",
      "epoch: 805 train loss: 0.018629134 epoch time: 2688.321 ms\n",
      "epoch: 806 train loss: 0.020712383 epoch time: 2486.583 ms\n",
      "epoch: 807 train loss: 0.033984266 epoch time: 2507.016 ms\n",
      "epoch: 808 train loss: 0.061532594 epoch time: 2539.407 ms\n",
      "epoch: 809 train loss: 0.1276592 epoch time: 2526.796 ms\n",
      "epoch: 810 train loss: 0.27670267 epoch time: 2483.319 ms\n",
      "epoch: 811 train loss: 0.6511615 epoch time: 2540.492 ms\n",
      "epoch: 812 train loss: 1.1590394 epoch time: 2470.360 ms\n",
      "epoch: 813 train loss: 0.8699328 epoch time: 2504.977 ms\n",
      "epoch: 814 train loss: 0.08610664 epoch time: 2483.897 ms\n",
      "epoch: 815 train loss: 0.16117938 epoch time: 2710.347 ms\n",
      "epoch: 816 train loss: 0.3564069 epoch time: 2809.380 ms\n",
      "epoch: 817 train loss: 0.12062258 epoch time: 2843.022 ms\n",
      "epoch: 818 train loss: 0.038083717 epoch time: 2884.643 ms\n",
      "epoch: 819 train loss: 0.07612628 epoch time: 2856.121 ms\n",
      "epoch: 820 train loss: 0.09741181 epoch time: 2809.451 ms\n",
      "epoch: 821 train loss: 0.07232877 epoch time: 2767.946 ms\n",
      "epoch: 822 train loss: 0.034702137 epoch time: 2826.946 ms\n",
      "epoch: 823 train loss: 0.03609571 epoch time: 2553.180 ms\n",
      "epoch: 824 train loss: 0.06825759 epoch time: 2502.297 ms\n",
      "epoch: 825 train loss: 0.111374065 epoch time: 2536.076 ms\n",
      "epoch: 826 train loss: 0.17364652 epoch time: 2593.519 ms\n",
      "epoch: 827 train loss: 0.24642374 epoch time: 2640.572 ms\n",
      "epoch: 828 train loss: 0.20473732 epoch time: 2693.203 ms\n",
      "epoch: 829 train loss: 0.24448511 epoch time: 2825.368 ms\n",
      "epoch: 830 train loss: 0.20810229 epoch time: 2639.402 ms\n",
      "epoch: 831 train loss: 0.21221298 epoch time: 2684.981 ms\n",
      "epoch: 832 train loss: 0.21931666 epoch time: 2820.306 ms\n",
      "epoch: 833 train loss: 0.17936033 epoch time: 2899.670 ms\n",
      "epoch: 834 train loss: 0.19093163 epoch time: 2773.929 ms\n",
      "epoch: 835 train loss: 0.1959708 epoch time: 2755.518 ms\n",
      "epoch: 836 train loss: 0.2599649 epoch time: 2861.100 ms\n",
      "epoch: 837 train loss: 0.3646156 epoch time: 2496.442 ms\n",
      "epoch: 838 train loss: 0.5380327 epoch time: 2573.200 ms\n",
      "epoch: 839 train loss: 0.5494395 epoch time: 2526.341 ms\n",
      "epoch: 840 train loss: 0.21235672 epoch time: 2569.494 ms\n",
      "epoch: 841 train loss: 0.103647895 epoch time: 2583.978 ms\n",
      "epoch: 842 train loss: 0.11531353 epoch time: 2483.653 ms\n",
      "epoch: 843 train loss: 0.033134863 epoch time: 2493.894 ms\n",
      "epoch: 844 train loss: 0.025251161 epoch time: 2523.349 ms\n",
      "epoch: 845 train loss: 0.057191625 epoch time: 2616.064 ms\n",
      "epoch: 846 train loss: 0.03382411 epoch time: 2551.571 ms\n",
      "epoch: 847 train loss: 0.02714438 epoch time: 2748.781 ms\n",
      "epoch: 848 train loss: 0.058354102 epoch time: 2859.183 ms\n",
      "epoch: 849 train loss: 0.115977615 epoch time: 2640.789 ms\n",
      "epoch: 850 train loss: 0.28166908 epoch time: 2534.488 ms\n",
      "epoch: 851 train loss: 0.9476476 epoch time: 2539.860 ms\n",
      "epoch: 852 train loss: 1.8598042 epoch time: 2554.640 ms\n",
      "epoch: 853 train loss: 0.8226685 epoch time: 2434.317 ms\n",
      "epoch: 854 train loss: 0.25571474 epoch time: 2638.391 ms\n",
      "epoch: 855 train loss: 0.6149909 epoch time: 2524.309 ms\n",
      "epoch: 856 train loss: 0.10532448 epoch time: 2682.703 ms\n",
      "epoch: 857 train loss: 0.23123573 epoch time: 2843.901 ms\n",
      "epoch: 858 train loss: 0.069423 epoch time: 2880.701 ms\n",
      "epoch: 859 train loss: 0.11771903 epoch time: 2806.951 ms\n",
      "epoch: 860 train loss: 0.04292563 epoch time: 2538.580 ms\n",
      "epoch: 861 train loss: 0.05065468 epoch time: 2536.927 ms\n",
      "epoch: 862 train loss: 0.040437657 epoch time: 2517.448 ms\n",
      "epoch: 863 train loss: 0.016084962 epoch time: 2513.612 ms\n",
      "epoch: 864 train loss: 0.042989507 epoch time: 2498.098 ms\n",
      "epoch: 865 train loss: 0.02053742 epoch time: 2548.286 ms\n",
      "epoch: 866 train loss: 0.01631823 epoch time: 2491.503 ms\n",
      "epoch: 867 train loss: 0.021512087 epoch time: 2494.298 ms\n",
      "epoch: 868 train loss: 0.01156923 epoch time: 2515.851 ms\n",
      "epoch: 869 train loss: 0.015404339 epoch time: 2426.207 ms\n",
      "epoch: 870 train loss: 0.014562786 epoch time: 2491.667 ms\n",
      "epoch: 871 train loss: 0.011943404 epoch time: 2507.260 ms\n",
      "epoch: 872 train loss: 0.014709778 epoch time: 2515.249 ms\n",
      "epoch: 873 train loss: 0.017710611 epoch time: 2871.759 ms\n",
      "epoch: 874 train loss: 0.032757223 epoch time: 2747.811 ms\n",
      "epoch: 875 train loss: 0.099411026 epoch time: 2801.070 ms\n",
      "epoch: 876 train loss: 0.3725042 epoch time: 2446.316 ms\n",
      "epoch: 877 train loss: 1.5486678 epoch time: 2478.085 ms\n",
      "epoch: 878 train loss: 2.7405996 epoch time: 2531.869 ms\n",
      "epoch: 879 train loss: 0.67935145 epoch time: 2523.502 ms\n",
      "epoch: 880 train loss: 0.77943385 epoch time: 2478.625 ms\n",
      "epoch: 881 train loss: 0.45071465 epoch time: 2512.783 ms\n",
      "epoch: 882 train loss: 0.25574258 epoch time: 2485.837 ms\n",
      "epoch: 883 train loss: 0.31074667 epoch time: 2540.007 ms\n",
      "epoch: 884 train loss: 0.033152796 epoch time: 2484.666 ms\n",
      "epoch: 885 train loss: 0.2119292 epoch time: 2522.915 ms\n",
      "epoch: 886 train loss: 0.025876388 epoch time: 2543.529 ms\n",
      "epoch: 887 train loss: 0.065912955 epoch time: 2646.325 ms\n",
      "epoch: 888 train loss: 0.03868974 epoch time: 2821.515 ms\n",
      "epoch: 889 train loss: 0.028960075 epoch time: 2718.579 ms\n",
      "epoch: 890 train loss: 0.035872295 epoch time: 2491.634 ms\n",
      "epoch: 891 train loss: 0.01567053 epoch time: 2831.997 ms\n",
      "epoch: 892 train loss: 0.025912227 epoch time: 2760.045 ms\n",
      "epoch: 893 train loss: 0.016946562 epoch time: 2759.064 ms\n",
      "epoch: 894 train loss: 0.012999138 epoch time: 2834.793 ms\n",
      "epoch: 895 train loss: 0.01822497 epoch time: 2817.961 ms\n",
      "epoch: 896 train loss: 0.012167979 epoch time: 2932.599 ms\n",
      "epoch: 897 train loss: 0.01274544 epoch time: 2825.556 ms\n",
      "epoch: 898 train loss: 0.011714865 epoch time: 2671.438 ms\n",
      "epoch: 899 train loss: 0.0115024205 epoch time: 2949.776 ms\n",
      "epoch: 900 train loss: 0.010832412 epoch time: 2876.336 ms\n",
      "    predict total time: 2.584218978881836 ms\n",
      "    l2_error:  0.015742955811128966\n",
      "==================================================================================================\n",
      "epoch: 901 train loss: 0.011633892 epoch time: 2859.277 ms\n",
      "epoch: 902 train loss: 0.012129253 epoch time: 2826.902 ms\n",
      "epoch: 903 train loss: 0.013043909 epoch time: 2735.047 ms\n",
      "epoch: 904 train loss: 0.020892039 epoch time: 2580.553 ms\n",
      "epoch: 905 train loss: 0.034566805 epoch time: 2855.767 ms\n",
      "epoch: 906 train loss: 0.06294475 epoch time: 2876.665 ms\n",
      "epoch: 907 train loss: 0.12232717 epoch time: 2502.573 ms\n",
      "epoch: 908 train loss: 0.33587041 epoch time: 2549.548 ms\n",
      "epoch: 909 train loss: 0.8860243 epoch time: 2509.964 ms\n",
      "epoch: 910 train loss: 1.1926221 epoch time: 2464.821 ms\n",
      "epoch: 911 train loss: 0.38768795 epoch time: 2493.410 ms\n",
      "epoch: 912 train loss: 0.017416148 epoch time: 2612.055 ms\n",
      "epoch: 913 train loss: 0.22915745 epoch time: 2647.606 ms\n",
      "epoch: 914 train loss: 0.2445868 epoch time: 2510.647 ms\n",
      "epoch: 915 train loss: 0.044215962 epoch time: 2581.599 ms\n",
      "epoch: 916 train loss: 0.053512186 epoch time: 2623.872 ms\n",
      "epoch: 917 train loss: 0.1408503 epoch time: 2560.094 ms\n",
      "epoch: 918 train loss: 0.11235625 epoch time: 2618.179 ms\n",
      "epoch: 919 train loss: 0.028776355 epoch time: 2693.286 ms\n",
      "epoch: 920 train loss: 0.018071247 epoch time: 2706.288 ms\n",
      "epoch: 921 train loss: 0.044424914 epoch time: 2487.663 ms\n",
      "epoch: 922 train loss: 0.044016976 epoch time: 2500.806 ms\n",
      "epoch: 923 train loss: 0.029264199 epoch time: 2491.030 ms\n",
      "epoch: 924 train loss: 0.017342336 epoch time: 2543.836 ms\n",
      "epoch: 925 train loss: 0.012104326 epoch time: 2730.828 ms\n",
      "epoch: 926 train loss: 0.011462319 epoch time: 2682.264 ms\n",
      "epoch: 927 train loss: 0.012182398 epoch time: 2510.010 ms\n",
      "epoch: 928 train loss: 0.015151394 epoch time: 2665.101 ms\n",
      "epoch: 929 train loss: 0.015979556 epoch time: 2748.397 ms\n",
      "epoch: 930 train loss: 0.026240837 epoch time: 2851.784 ms\n",
      "epoch: 931 train loss: 0.08319872 epoch time: 3000.005 ms\n",
      "epoch: 932 train loss: 0.31452918 epoch time: 2881.142 ms\n",
      "epoch: 933 train loss: 1.394998 epoch time: 2867.541 ms\n",
      "epoch: 934 train loss: 3.1202059 epoch time: 2789.250 ms\n",
      "epoch: 935 train loss: 0.6227013 epoch time: 2877.980 ms\n",
      "epoch: 936 train loss: 1.5059998 epoch time: 2666.780 ms\n",
      "epoch: 937 train loss: 0.14992924 epoch time: 2876.917 ms\n",
      "epoch: 938 train loss: 0.6064836 epoch time: 2737.548 ms\n",
      "epoch: 939 train loss: 0.19827452 epoch time: 2801.134 ms\n",
      "epoch: 940 train loss: 0.07354664 epoch time: 2791.573 ms\n",
      "epoch: 941 train loss: 0.19770595 epoch time: 2812.125 ms\n",
      "epoch: 942 train loss: 0.051435985 epoch time: 2881.677 ms\n",
      "epoch: 943 train loss: 0.031098135 epoch time: 3052.291 ms\n",
      "epoch: 944 train loss: 0.06538682 epoch time: 2738.132 ms\n",
      "epoch: 945 train loss: 0.018030891 epoch time: 2886.006 ms\n",
      "epoch: 946 train loss: 0.023652585 epoch time: 2779.350 ms\n",
      "epoch: 947 train loss: 0.022258628 epoch time: 2814.219 ms\n",
      "epoch: 948 train loss: 0.010008534 epoch time: 2749.627 ms\n",
      "epoch: 949 train loss: 0.016838836 epoch time: 2698.685 ms\n",
      "epoch: 950 train loss: 0.011393236 epoch time: 2486.027 ms\n",
      "epoch: 951 train loss: 0.011850368 epoch time: 2599.410 ms\n",
      "epoch: 952 train loss: 0.010176889 epoch time: 2876.745 ms\n",
      "epoch: 953 train loss: 0.009819508 epoch time: 2653.309 ms\n",
      "epoch: 954 train loss: 0.009719651 epoch time: 2636.993 ms\n",
      "epoch: 955 train loss: 0.010726074 epoch time: 2759.936 ms\n",
      "epoch: 956 train loss: 0.01033748 epoch time: 2620.525 ms\n",
      "epoch: 957 train loss: 0.010705633 epoch time: 2732.204 ms\n",
      "epoch: 958 train loss: 0.009911476 epoch time: 2811.658 ms\n",
      "epoch: 959 train loss: 0.009972349 epoch time: 2772.146 ms\n",
      "epoch: 960 train loss: 0.0094390055 epoch time: 2863.368 ms\n",
      "epoch: 961 train loss: 0.009210228 epoch time: 2711.298 ms\n",
      "epoch: 962 train loss: 0.010345964 epoch time: 2859.156 ms\n",
      "epoch: 963 train loss: 0.00971683 epoch time: 2649.652 ms\n",
      "epoch: 964 train loss: 0.011110722 epoch time: 2842.621 ms\n",
      "epoch: 965 train loss: 0.012429612 epoch time: 2750.715 ms\n",
      "epoch: 966 train loss: 0.018698419 epoch time: 2853.336 ms\n",
      "epoch: 967 train loss: 0.038129285 epoch time: 2687.698 ms\n",
      "epoch: 968 train loss: 0.15907644 epoch time: 2793.299 ms\n",
      "epoch: 969 train loss: 0.7819212 epoch time: 2713.742 ms\n",
      "epoch: 970 train loss: 3.2553174 epoch time: 2727.681 ms\n",
      "epoch: 971 train loss: 2.6504877 epoch time: 2539.151 ms\n",
      "epoch: 972 train loss: 0.82678497 epoch time: 2461.874 ms\n",
      "epoch: 973 train loss: 0.5791975 epoch time: 2488.955 ms\n",
      "epoch: 974 train loss: 0.6375698 epoch time: 2818.567 ms\n",
      "epoch: 975 train loss: 0.24535154 epoch time: 2894.220 ms\n",
      "epoch: 976 train loss: 0.12854464 epoch time: 2853.875 ms\n",
      "epoch: 977 train loss: 0.16493142 epoch time: 2762.242 ms\n",
      "epoch: 978 train loss: 0.148573 epoch time: 2693.348 ms\n",
      "epoch: 979 train loss: 0.024682794 epoch time: 2869.080 ms\n",
      "epoch: 980 train loss: 0.05955986 epoch time: 2648.983 ms\n",
      "epoch: 981 train loss: 0.036480326 epoch time: 2851.767 ms\n",
      "epoch: 982 train loss: 0.03838924 epoch time: 2917.040 ms\n",
      "epoch: 983 train loss: 0.013176162 epoch time: 2941.837 ms\n",
      "epoch: 984 train loss: 0.021925058 epoch time: 2911.454 ms\n",
      "epoch: 985 train loss: 0.015145145 epoch time: 2778.610 ms\n",
      "epoch: 986 train loss: 0.01192223 epoch time: 2628.393 ms\n",
      "epoch: 987 train loss: 0.0119152 epoch time: 2687.318 ms\n",
      "epoch: 988 train loss: 0.010241643 epoch time: 3009.452 ms\n",
      "epoch: 989 train loss: 0.011244729 epoch time: 2676.006 ms\n",
      "epoch: 990 train loss: 0.011830218 epoch time: 2534.723 ms\n",
      "epoch: 991 train loss: 0.010552448 epoch time: 2471.984 ms\n",
      "epoch: 992 train loss: 0.009772076 epoch time: 2500.412 ms\n",
      "epoch: 993 train loss: 0.0100400755 epoch time: 2530.028 ms\n",
      "epoch: 994 train loss: 0.010043194 epoch time: 2504.775 ms\n",
      "epoch: 995 train loss: 0.009206062 epoch time: 2447.814 ms\n",
      "epoch: 996 train loss: 0.009931466 epoch time: 2544.471 ms\n",
      "epoch: 997 train loss: 0.0094319945 epoch time: 2583.760 ms\n",
      "epoch: 998 train loss: 0.010206033 epoch time: 2522.578 ms\n",
      "epoch: 999 train loss: 0.011297912 epoch time: 2479.699 ms\n",
      "epoch: 1000 train loss: 0.011483245 epoch time: 2557.783 ms\n",
      "    predict total time: 2.4895668029785156 ms\n",
      "    l2_error:  0.01525962606777051\n",
      "==================================================================================================\n",
      "epoch: 1001 train loss: 0.009791969 epoch time: 2513.122 ms\n",
      "epoch: 1002 train loss: 0.009599962 epoch time: 2506.985 ms\n",
      "epoch: 1003 train loss: 0.011535431 epoch time: 2479.338 ms\n",
      "epoch: 1004 train loss: 0.019617885 epoch time: 2756.058 ms\n",
      "epoch: 1005 train loss: 0.05439116 epoch time: 2821.923 ms\n",
      "epoch: 1006 train loss: 0.17517003 epoch time: 2884.095 ms\n",
      "epoch: 1007 train loss: 0.46480614 epoch time: 2831.415 ms\n",
      "epoch: 1008 train loss: 0.7426706 epoch time: 2934.375 ms\n",
      "epoch: 1009 train loss: 0.89723295 epoch time: 2779.790 ms\n",
      "epoch: 1010 train loss: 0.26308975 epoch time: 2731.319 ms\n",
      "epoch: 1011 train loss: 0.04712317 epoch time: 2574.749 ms\n",
      "epoch: 1012 train loss: 0.2798701 epoch time: 2816.781 ms\n",
      "epoch: 1013 train loss: 0.18870403 epoch time: 2578.046 ms\n",
      "epoch: 1014 train loss: 0.024268383 epoch time: 2466.532 ms\n",
      "epoch: 1015 train loss: 0.03882335 epoch time: 2521.842 ms\n",
      "epoch: 1016 train loss: 0.08200731 epoch time: 2558.672 ms\n",
      "epoch: 1017 train loss: 0.05623932 epoch time: 2462.932 ms\n",
      "epoch: 1018 train loss: 0.015516083 epoch time: 2468.622 ms\n",
      "epoch: 1019 train loss: 0.01246451 epoch time: 2480.870 ms\n",
      "epoch: 1020 train loss: 0.025164098 epoch time: 2528.014 ms\n",
      "epoch: 1021 train loss: 0.038094155 epoch time: 2577.056 ms\n",
      "epoch: 1022 train loss: 0.039688837 epoch time: 2790.433 ms\n",
      "epoch: 1023 train loss: 0.030568644 epoch time: 2858.764 ms\n",
      "epoch: 1024 train loss: 0.023235751 epoch time: 2846.618 ms\n",
      "epoch: 1025 train loss: 0.023201846 epoch time: 2749.042 ms\n",
      "epoch: 1026 train loss: 0.038969368 epoch time: 2882.783 ms\n",
      "epoch: 1027 train loss: 0.09123278 epoch time: 2729.576 ms\n",
      "epoch: 1028 train loss: 0.2520247 epoch time: 2835.720 ms\n",
      "epoch: 1029 train loss: 0.8225891 epoch time: 2946.073 ms\n",
      "epoch: 1030 train loss: 1.6528506 epoch time: 2725.957 ms\n",
      "epoch: 1031 train loss: 0.6340565 epoch time: 2637.038 ms\n",
      "epoch: 1032 train loss: 0.6125816 epoch time: 2981.058 ms\n",
      "epoch: 1033 train loss: 0.1219237 epoch time: 2874.251 ms\n",
      "epoch: 1034 train loss: 0.40006304 epoch time: 2824.084 ms\n",
      "epoch: 1035 train loss: 0.06813205 epoch time: 2686.333 ms\n",
      "epoch: 1036 train loss: 0.10051718 epoch time: 2825.028 ms\n",
      "epoch: 1037 train loss: 0.17565517 epoch time: 2825.660 ms\n",
      "epoch: 1038 train loss: 0.06538906 epoch time: 2882.123 ms\n",
      "epoch: 1039 train loss: 0.045144066 epoch time: 2818.316 ms\n",
      "epoch: 1040 train loss: 0.029350135 epoch time: 2712.370 ms\n",
      "epoch: 1041 train loss: 0.02990144 epoch time: 2755.867 ms\n",
      "epoch: 1042 train loss: 0.020079091 epoch time: 3006.249 ms\n",
      "epoch: 1043 train loss: 0.014818302 epoch time: 2959.767 ms\n",
      "epoch: 1044 train loss: 0.013013104 epoch time: 2934.296 ms\n",
      "epoch: 1045 train loss: 0.008898787 epoch time: 2733.220 ms\n",
      "epoch: 1046 train loss: 0.009820464 epoch time: 2790.612 ms\n",
      "epoch: 1047 train loss: 0.008455734 epoch time: 2732.076 ms\n",
      "epoch: 1048 train loss: 0.009677351 epoch time: 2904.257 ms\n",
      "epoch: 1049 train loss: 0.010925771 epoch time: 2857.262 ms\n",
      "epoch: 1050 train loss: 0.018853407 epoch time: 2827.541 ms\n",
      "epoch: 1051 train loss: 0.05292396 epoch time: 2792.708 ms\n",
      "epoch: 1052 train loss: 0.21194836 epoch time: 2631.838 ms\n",
      "epoch: 1053 train loss: 0.9187166 epoch time: 2699.536 ms\n",
      "epoch: 1054 train loss: 2.4321368 epoch time: 2852.216 ms\n",
      "epoch: 1055 train loss: 1.0751693 epoch time: 2706.051 ms\n",
      "epoch: 1056 train loss: 0.21966124 epoch time: 2844.808 ms\n",
      "epoch: 1057 train loss: 0.6842116 epoch time: 2765.577 ms\n",
      "epoch: 1058 train loss: 0.053690642 epoch time: 2875.623 ms\n",
      "epoch: 1059 train loss: 0.36328167 epoch time: 2857.382 ms\n",
      "epoch: 1060 train loss: 0.02144337 epoch time: 2794.914 ms\n",
      "epoch: 1061 train loss: 0.15885857 epoch time: 2717.726 ms\n",
      "epoch: 1062 train loss: 0.029952152 epoch time: 2927.132 ms\n",
      "epoch: 1063 train loss: 0.07452291 epoch time: 2772.256 ms\n",
      "epoch: 1064 train loss: 0.021993337 epoch time: 2476.827 ms\n",
      "epoch: 1065 train loss: 0.034953523 epoch time: 2587.315 ms\n",
      "epoch: 1066 train loss: 0.017010577 epoch time: 2687.446 ms\n",
      "epoch: 1067 train loss: 0.02735586 epoch time: 2581.215 ms\n",
      "epoch: 1068 train loss: 0.011001907 epoch time: 2768.400 ms\n",
      "epoch: 1069 train loss: 0.015408289 epoch time: 2478.457 ms\n",
      "epoch: 1070 train loss: 0.011613141 epoch time: 2720.873 ms\n",
      "epoch: 1071 train loss: 0.008968021 epoch time: 3043.060 ms\n",
      "epoch: 1072 train loss: 0.011265533 epoch time: 3094.388 ms\n",
      "epoch: 1073 train loss: 0.010783375 epoch time: 2802.706 ms\n",
      "epoch: 1074 train loss: 0.0083322115 epoch time: 2681.961 ms\n",
      "epoch: 1075 train loss: 0.009117784 epoch time: 2508.653 ms\n",
      "epoch: 1076 train loss: 0.009541839 epoch time: 2879.247 ms\n",
      "epoch: 1077 train loss: 0.013010245 epoch time: 2755.081 ms\n",
      "epoch: 1078 train loss: 0.018512744 epoch time: 2742.903 ms\n",
      "epoch: 1079 train loss: 0.024395369 epoch time: 2485.184 ms\n",
      "epoch: 1080 train loss: 0.024426201 epoch time: 2503.890 ms\n",
      "epoch: 1081 train loss: 0.028971503 epoch time: 2507.590 ms\n",
      "epoch: 1082 train loss: 0.0394351 epoch time: 2808.383 ms\n",
      "epoch: 1083 train loss: 0.060669955 epoch time: 3030.298 ms\n",
      "epoch: 1084 train loss: 0.13712178 epoch time: 2801.073 ms\n",
      "epoch: 1085 train loss: 0.36575168 epoch time: 2748.651 ms\n",
      "epoch: 1086 train loss: 1.0133833 epoch time: 2677.929 ms\n",
      "epoch: 1087 train loss: 1.466332 epoch time: 2708.829 ms\n",
      "epoch: 1088 train loss: 0.18571474 epoch time: 2810.502 ms\n",
      "epoch: 1089 train loss: 0.41770956 epoch time: 2814.187 ms\n",
      "epoch: 1090 train loss: 0.5245976 epoch time: 2844.949 ms\n",
      "epoch: 1091 train loss: 0.031539213 epoch time: 2839.695 ms\n",
      "epoch: 1092 train loss: 0.24499117 epoch time: 2713.487 ms\n",
      "epoch: 1093 train loss: 0.11736502 epoch time: 2741.830 ms\n",
      "epoch: 1094 train loss: 0.021172442 epoch time: 2754.727 ms\n",
      "epoch: 1095 train loss: 0.113562375 epoch time: 2714.294 ms\n",
      "epoch: 1096 train loss: 0.067862034 epoch time: 2770.890 ms\n",
      "epoch: 1097 train loss: 0.06560549 epoch time: 2779.281 ms\n",
      "epoch: 1098 train loss: 0.094789185 epoch time: 2643.908 ms\n",
      "epoch: 1099 train loss: 0.07566092 epoch time: 2513.903 ms\n",
      "epoch: 1100 train loss: 0.12625839 epoch time: 2805.003 ms\n",
      "    predict total time: 2.636432647705078 ms\n",
      "    l2_error:  0.02234665005262076\n",
      "==================================================================================================\n",
      "epoch: 1101 train loss: 0.22693157 epoch time: 2750.062 ms\n",
      "epoch: 1102 train loss: 0.3010503 epoch time: 2826.529 ms\n",
      "epoch: 1103 train loss: 0.18813756 epoch time: 2636.061 ms\n",
      "epoch: 1104 train loss: 0.012673572 epoch time: 2580.806 ms\n",
      "epoch: 1105 train loss: 0.08549323 epoch time: 2673.431 ms\n",
      "epoch: 1106 train loss: 0.083011046 epoch time: 2571.868 ms\n",
      "epoch: 1107 train loss: 0.008885086 epoch time: 2619.786 ms\n",
      "epoch: 1108 train loss: 0.040818367 epoch time: 2613.448 ms\n",
      "epoch: 1109 train loss: 0.047106087 epoch time: 2752.892 ms\n",
      "epoch: 1110 train loss: 0.013032602 epoch time: 2686.666 ms\n",
      "epoch: 1111 train loss: 0.017690986 epoch time: 2728.314 ms\n",
      "epoch: 1112 train loss: 0.035437714 epoch time: 2880.934 ms\n",
      "epoch: 1113 train loss: 0.022751804 epoch time: 2800.056 ms\n",
      "epoch: 1114 train loss: 0.014910333 epoch time: 2509.933 ms\n",
      "epoch: 1115 train loss: 0.028283468 epoch time: 2638.930 ms\n",
      "epoch: 1116 train loss: 0.14269596 epoch time: 2674.121 ms\n",
      "epoch: 1117 train loss: 0.66215116 epoch time: 2553.420 ms\n",
      "epoch: 1118 train loss: 2.2417018 epoch time: 2801.856 ms\n",
      "epoch: 1119 train loss: 2.658369 epoch time: 2790.508 ms\n",
      "epoch: 1120 train loss: 0.26755401 epoch time: 2763.287 ms\n",
      "epoch: 1121 train loss: 1.4227983 epoch time: 2884.787 ms\n",
      "epoch: 1122 train loss: 0.3398467 epoch time: 2966.415 ms\n",
      "epoch: 1123 train loss: 0.33850902 epoch time: 2743.902 ms\n",
      "epoch: 1124 train loss: 0.3895692 epoch time: 2876.431 ms\n",
      "epoch: 1125 train loss: 0.020610636 epoch time: 2695.844 ms\n",
      "epoch: 1126 train loss: 0.13886738 epoch time: 2868.821 ms\n",
      "epoch: 1127 train loss: 0.068583466 epoch time: 2830.889 ms\n",
      "epoch: 1128 train loss: 0.025563808 epoch time: 2697.786 ms\n",
      "epoch: 1129 train loss: 0.054977383 epoch time: 2820.431 ms\n",
      "epoch: 1130 train loss: 0.009438102 epoch time: 2760.628 ms\n",
      "epoch: 1131 train loss: 0.030106768 epoch time: 2815.382 ms\n",
      "epoch: 1132 train loss: 0.014975163 epoch time: 2974.986 ms\n",
      "epoch: 1133 train loss: 0.015122334 epoch time: 2785.032 ms\n",
      "epoch: 1134 train loss: 0.014340607 epoch time: 2943.692 ms\n",
      "epoch: 1135 train loss: 0.013493328 epoch time: 2965.733 ms\n",
      "epoch: 1136 train loss: 0.012227479 epoch time: 2741.413 ms\n",
      "epoch: 1137 train loss: 0.008454673 epoch time: 2849.196 ms\n",
      "epoch: 1138 train loss: 0.008965969 epoch time: 2588.833 ms\n",
      "epoch: 1139 train loss: 0.012751586 epoch time: 2498.130 ms\n",
      "epoch: 1140 train loss: 0.015490512 epoch time: 2600.934 ms\n",
      "epoch: 1141 train loss: 0.019129734 epoch time: 2720.068 ms\n",
      "epoch: 1142 train loss: 0.02423161 epoch time: 2791.336 ms\n",
      "epoch: 1143 train loss: 0.03843544 epoch time: 2973.781 ms\n",
      "epoch: 1144 train loss: 0.08159122 epoch time: 2778.572 ms\n",
      "epoch: 1145 train loss: 0.22279847 epoch time: 2653.903 ms\n",
      "epoch: 1146 train loss: 0.42584336 epoch time: 2677.850 ms\n",
      "epoch: 1147 train loss: 0.21779308 epoch time: 2891.091 ms\n",
      "epoch: 1148 train loss: 0.010558372 epoch time: 2760.435 ms\n",
      "epoch: 1149 train loss: 0.13639936 epoch time: 2604.202 ms\n",
      "epoch: 1150 train loss: 0.07630566 epoch time: 2713.881 ms\n",
      "epoch: 1151 train loss: 0.024044063 epoch time: 2800.509 ms\n",
      "epoch: 1152 train loss: 0.048636995 epoch time: 3097.226 ms\n",
      "epoch: 1153 train loss: 0.025924254 epoch time: 2818.116 ms\n",
      "epoch: 1154 train loss: 0.026799288 epoch time: 2914.122 ms\n",
      "epoch: 1155 train loss: 0.031417333 epoch time: 2712.267 ms\n",
      "epoch: 1156 train loss: 0.013755403 epoch time: 2953.140 ms\n",
      "epoch: 1157 train loss: 0.008330757 epoch time: 2763.654 ms\n",
      "epoch: 1158 train loss: 0.014042408 epoch time: 3078.815 ms\n",
      "epoch: 1159 train loss: 0.016036153 epoch time: 2959.493 ms\n",
      "epoch: 1160 train loss: 0.015049006 epoch time: 2886.310 ms\n",
      "epoch: 1161 train loss: 0.011355728 epoch time: 2595.601 ms\n",
      "epoch: 1162 train loss: 0.014653659 epoch time: 2681.771 ms\n",
      "epoch: 1163 train loss: 0.026859518 epoch time: 2663.434 ms\n",
      "epoch: 1164 train loss: 0.059622277 epoch time: 2917.290 ms\n",
      "epoch: 1165 train loss: 0.18663928 epoch time: 2879.386 ms\n",
      "epoch: 1166 train loss: 0.5666704 epoch time: 2845.790 ms\n",
      "epoch: 1167 train loss: 1.3154286 epoch time: 2847.672 ms\n",
      "epoch: 1168 train loss: 0.8860312 epoch time: 2755.228 ms\n",
      "epoch: 1169 train loss: 0.39576477 epoch time: 2723.078 ms\n",
      "epoch: 1170 train loss: 0.5161139 epoch time: 2770.533 ms\n",
      "epoch: 1171 train loss: 0.081730574 epoch time: 2812.067 ms\n",
      "epoch: 1172 train loss: 0.23683962 epoch time: 2872.803 ms\n",
      "epoch: 1173 train loss: 0.030390522 epoch time: 2635.598 ms\n",
      "epoch: 1174 train loss: 0.10772786 epoch time: 2681.077 ms\n",
      "epoch: 1175 train loss: 0.019494794 epoch time: 2676.944 ms\n",
      "epoch: 1176 train loss: 0.05384712 epoch time: 2646.878 ms\n",
      "epoch: 1177 train loss: 0.0151231345 epoch time: 2543.628 ms\n",
      "epoch: 1178 train loss: 0.024850264 epoch time: 2566.285 ms\n",
      "epoch: 1179 train loss: 0.020053852 epoch time: 2635.076 ms\n",
      "epoch: 1180 train loss: 0.024615709 epoch time: 2794.656 ms\n",
      "epoch: 1181 train loss: 0.023189912 epoch time: 2932.805 ms\n",
      "epoch: 1182 train loss: 0.03062201 epoch time: 2810.766 ms\n",
      "epoch: 1183 train loss: 0.049927443 epoch time: 2938.757 ms\n",
      "epoch: 1184 train loss: 0.11592022 epoch time: 2642.381 ms\n",
      "epoch: 1185 train loss: 0.3184719 epoch time: 2947.485 ms\n",
      "epoch: 1186 train loss: 1.0925024 epoch time: 2902.629 ms\n",
      "epoch: 1187 train loss: 1.7017868 epoch time: 2694.078 ms\n",
      "epoch: 1188 train loss: 0.3164514 epoch time: 2640.450 ms\n",
      "epoch: 1189 train loss: 0.2017783 epoch time: 2880.321 ms\n",
      "epoch: 1190 train loss: 0.41249445 epoch time: 2989.775 ms\n",
      "epoch: 1191 train loss: 0.030580604 epoch time: 2814.167 ms\n",
      "epoch: 1192 train loss: 0.1269192 epoch time: 2811.666 ms\n",
      "epoch: 1193 train loss: 0.06917229 epoch time: 2915.909 ms\n",
      "epoch: 1194 train loss: 0.015575285 epoch time: 2801.855 ms\n",
      "epoch: 1195 train loss: 0.05292044 epoch time: 2600.618 ms\n",
      "epoch: 1196 train loss: 0.026772065 epoch time: 2711.607 ms\n",
      "epoch: 1197 train loss: 0.011081462 epoch time: 2944.608 ms\n",
      "epoch: 1198 train loss: 0.018390233 epoch time: 2738.199 ms\n",
      "epoch: 1199 train loss: 0.016528543 epoch time: 2780.409 ms\n",
      "epoch: 1200 train loss: 0.008483397 epoch time: 2641.402 ms\n",
      "    predict total time: 2.4993419647216797 ms\n",
      "    l2_error:  0.013807685602108048\n",
      "==================================================================================================\n",
      "epoch: 1201 train loss: 0.01063002 epoch time: 2741.590 ms\n",
      "epoch: 1202 train loss: 0.017610904 epoch time: 2794.771 ms\n",
      "epoch: 1203 train loss: 0.013795605 epoch time: 2684.167 ms\n",
      "epoch: 1204 train loss: 0.007841561 epoch time: 2948.003 ms\n",
      "epoch: 1205 train loss: 0.008463953 epoch time: 2861.824 ms\n",
      "epoch: 1206 train loss: 0.009883389 epoch time: 2814.885 ms\n",
      "epoch: 1207 train loss: 0.013603758 epoch time: 3043.832 ms\n",
      "epoch: 1208 train loss: 0.024443164 epoch time: 2869.000 ms\n",
      "epoch: 1209 train loss: 0.043373317 epoch time: 2659.092 ms\n",
      "epoch: 1210 train loss: 0.09391803 epoch time: 2733.159 ms\n",
      "epoch: 1211 train loss: 0.32649705 epoch time: 2699.650 ms\n",
      "epoch: 1212 train loss: 0.93874025 epoch time: 2679.364 ms\n",
      "epoch: 1213 train loss: 1.6023986 epoch time: 2520.963 ms\n",
      "epoch: 1214 train loss: 0.5165755 epoch time: 2606.420 ms\n",
      "epoch: 1215 train loss: 0.58647776 epoch time: 2698.651 ms\n",
      "epoch: 1216 train loss: 0.16910762 epoch time: 2832.325 ms\n",
      "epoch: 1217 train loss: 0.21522863 epoch time: 2691.291 ms\n",
      "epoch: 1218 train loss: 0.06282537 epoch time: 2857.175 ms\n",
      "epoch: 1219 train loss: 0.10188408 epoch time: 2811.575 ms\n",
      "epoch: 1220 train loss: 0.053002186 epoch time: 2733.985 ms\n",
      "epoch: 1221 train loss: 0.05339524 epoch time: 2838.791 ms\n",
      "epoch: 1222 train loss: 0.021656716 epoch time: 2910.775 ms\n",
      "epoch: 1223 train loss: 0.028453708 epoch time: 2935.936 ms\n",
      "epoch: 1224 train loss: 0.021316208 epoch time: 2930.037 ms\n",
      "epoch: 1225 train loss: 0.017609473 epoch time: 3016.122 ms\n",
      "epoch: 1226 train loss: 0.01953858 epoch time: 2907.024 ms\n",
      "epoch: 1227 train loss: 0.008080237 epoch time: 2997.583 ms\n",
      "epoch: 1228 train loss: 0.014781706 epoch time: 2914.422 ms\n",
      "epoch: 1229 train loss: 0.008672791 epoch time: 2665.615 ms\n",
      "epoch: 1230 train loss: 0.018495642 epoch time: 2890.564 ms\n",
      "epoch: 1231 train loss: 0.021780916 epoch time: 2977.485 ms\n",
      "epoch: 1232 train loss: 0.0288234 epoch time: 2738.975 ms\n",
      "epoch: 1233 train loss: 0.06799997 epoch time: 2813.074 ms\n",
      "epoch: 1234 train loss: 0.19298981 epoch time: 2789.337 ms\n",
      "epoch: 1235 train loss: 0.61716735 epoch time: 2838.342 ms\n",
      "epoch: 1236 train loss: 1.693548 epoch time: 2815.839 ms\n",
      "epoch: 1237 train loss: 1.9276588 epoch time: 2621.945 ms\n",
      "epoch: 1238 train loss: 0.074546754 epoch time: 2817.867 ms\n",
      "epoch: 1239 train loss: 0.7525632 epoch time: 2933.310 ms\n",
      "epoch: 1240 train loss: 0.09380767 epoch time: 2946.162 ms\n",
      "epoch: 1241 train loss: 0.2869949 epoch time: 2887.154 ms\n",
      "epoch: 1242 train loss: 0.112495385 epoch time: 2977.543 ms\n",
      "epoch: 1243 train loss: 0.0749533 epoch time: 2680.423 ms\n",
      "epoch: 1244 train loss: 0.10648951 epoch time: 2769.383 ms\n",
      "epoch: 1245 train loss: 0.0153221125 epoch time: 2781.312 ms\n",
      "epoch: 1246 train loss: 0.054167423 epoch time: 2714.194 ms\n",
      "epoch: 1247 train loss: 0.012465003 epoch time: 2974.894 ms\n",
      "epoch: 1248 train loss: 0.022318045 epoch time: 2975.525 ms\n",
      "epoch: 1249 train loss: 0.011353874 epoch time: 2744.276 ms\n",
      "epoch: 1250 train loss: 0.013319191 epoch time: 2790.261 ms\n",
      "epoch: 1251 train loss: 0.007950784 epoch time: 2542.798 ms\n",
      "epoch: 1252 train loss: 0.013046551 epoch time: 2466.008 ms\n",
      "epoch: 1253 train loss: 0.00846513 epoch time: 2515.827 ms\n",
      "epoch: 1254 train loss: 0.0081266 epoch time: 2735.855 ms\n",
      "epoch: 1255 train loss: 0.008865161 epoch time: 2868.800 ms\n",
      "epoch: 1256 train loss: 0.010492628 epoch time: 2776.567 ms\n",
      "epoch: 1257 train loss: 0.011929265 epoch time: 2499.557 ms\n",
      "epoch: 1258 train loss: 0.013651246 epoch time: 2527.251 ms\n",
      "epoch: 1259 train loss: 0.016849864 epoch time: 2564.727 ms\n",
      "epoch: 1260 train loss: 0.0259243 epoch time: 2513.254 ms\n",
      "epoch: 1261 train loss: 0.040805873 epoch time: 2482.670 ms\n",
      "epoch: 1262 train loss: 0.08385794 epoch time: 2488.957 ms\n",
      "epoch: 1263 train loss: 0.1911736 epoch time: 2576.592 ms\n",
      "epoch: 1264 train loss: 0.45422074 epoch time: 2547.145 ms\n",
      "epoch: 1265 train loss: 0.90130234 epoch time: 2806.959 ms\n",
      "epoch: 1266 train loss: 0.6640166 epoch time: 2693.447 ms\n",
      "epoch: 1267 train loss: 0.270651 epoch time: 2893.202 ms\n",
      "epoch: 1268 train loss: 0.10552244 epoch time: 2819.642 ms\n",
      "epoch: 1269 train loss: 0.22189455 epoch time: 2819.825 ms\n",
      "epoch: 1270 train loss: 0.09710554 epoch time: 2744.703 ms\n",
      "epoch: 1271 train loss: 0.034363784 epoch time: 2931.671 ms\n",
      "epoch: 1272 train loss: 0.09039841 epoch time: 2778.110 ms\n",
      "epoch: 1273 train loss: 0.04875573 epoch time: 2517.889 ms\n",
      "epoch: 1274 train loss: 0.026963959 epoch time: 2474.265 ms\n",
      "epoch: 1275 train loss: 0.010501197 epoch time: 2618.496 ms\n",
      "epoch: 1276 train loss: 0.010172452 epoch time: 2841.380 ms\n",
      "epoch: 1277 train loss: 0.015983466 epoch time: 2653.493 ms\n",
      "epoch: 1278 train loss: 0.019059535 epoch time: 2793.212 ms\n",
      "epoch: 1279 train loss: 0.025181018 epoch time: 2860.219 ms\n",
      "epoch: 1280 train loss: 0.050572757 epoch time: 2887.087 ms\n",
      "epoch: 1281 train loss: 0.08985699 epoch time: 2672.498 ms\n",
      "epoch: 1282 train loss: 0.17648864 epoch time: 2802.270 ms\n",
      "epoch: 1283 train loss: 0.3492183 epoch time: 2875.836 ms\n",
      "epoch: 1284 train loss: 0.7045009 epoch time: 2866.887 ms\n",
      "epoch: 1285 train loss: 0.8243905 epoch time: 2980.046 ms\n",
      "epoch: 1286 train loss: 0.20611808 epoch time: 2706.842 ms\n",
      "epoch: 1287 train loss: 0.061775032 epoch time: 2896.528 ms\n",
      "epoch: 1288 train loss: 0.2734441 epoch time: 2734.912 ms\n",
      "epoch: 1289 train loss: 0.21869369 epoch time: 2811.651 ms\n",
      "epoch: 1290 train loss: 0.033956233 epoch time: 2610.179 ms\n",
      "epoch: 1291 train loss: 0.071344845 epoch time: 2548.790 ms\n",
      "epoch: 1292 train loss: 0.090908505 epoch time: 2475.095 ms\n",
      "epoch: 1293 train loss: 0.05867206 epoch time: 2491.269 ms\n",
      "epoch: 1294 train loss: 0.017425504 epoch time: 2773.371 ms\n",
      "epoch: 1295 train loss: 0.013786372 epoch time: 2684.731 ms\n",
      "epoch: 1296 train loss: 0.028589703 epoch time: 2581.815 ms\n",
      "epoch: 1297 train loss: 0.038275033 epoch time: 2742.749 ms\n",
      "epoch: 1298 train loss: 0.041462034 epoch time: 2729.001 ms\n",
      "epoch: 1299 train loss: 0.053301778 epoch time: 2706.530 ms\n",
      "epoch: 1300 train loss: 0.109358065 epoch time: 2774.696 ms\n",
      "    predict total time: 2.5281906127929688 ms\n",
      "    l2_error:  0.01925940027718034\n",
      "==================================================================================================\n",
      "epoch: 1301 train loss: 0.21430163 epoch time: 2678.312 ms\n",
      "epoch: 1302 train loss: 0.33452058 epoch time: 2534.508 ms\n",
      "epoch: 1303 train loss: 0.5176723 epoch time: 2493.057 ms\n",
      "epoch: 1304 train loss: 1.0496336 epoch time: 2795.929 ms\n",
      "epoch: 1305 train loss: 1.1771693 epoch time: 2712.519 ms\n",
      "epoch: 1306 train loss: 0.13945904 epoch time: 2834.239 ms\n",
      "epoch: 1307 train loss: 0.2698479 epoch time: 2826.798 ms\n",
      "epoch: 1308 train loss: 0.32999602 epoch time: 2679.638 ms\n",
      "epoch: 1309 train loss: 0.023352545 epoch time: 2923.894 ms\n",
      "epoch: 1310 train loss: 0.10542238 epoch time: 2765.819 ms\n",
      "epoch: 1311 train loss: 0.08465788 epoch time: 2782.887 ms\n",
      "epoch: 1312 train loss: 0.013328931 epoch time: 2603.939 ms\n",
      "epoch: 1313 train loss: 0.037960652 epoch time: 2824.993 ms\n",
      "epoch: 1314 train loss: 0.03116747 epoch time: 2854.023 ms\n",
      "epoch: 1315 train loss: 0.009396734 epoch time: 2800.496 ms\n",
      "epoch: 1316 train loss: 0.009922934 epoch time: 2595.876 ms\n",
      "epoch: 1317 train loss: 0.016845386 epoch time: 2572.705 ms\n",
      "epoch: 1318 train loss: 0.012859817 epoch time: 2745.595 ms\n",
      "epoch: 1319 train loss: 0.009310411 epoch time: 2885.689 ms\n",
      "epoch: 1320 train loss: 0.014778448 epoch time: 2721.928 ms\n",
      "epoch: 1321 train loss: 0.029056761 epoch time: 2546.248 ms\n",
      "epoch: 1322 train loss: 0.06276717 epoch time: 2951.762 ms\n",
      "epoch: 1323 train loss: 0.08215967 epoch time: 2824.732 ms\n",
      "epoch: 1324 train loss: 0.1383291 epoch time: 3001.374 ms\n",
      "epoch: 1325 train loss: 0.26002386 epoch time: 2818.719 ms\n",
      "epoch: 1326 train loss: 0.4042618 epoch time: 2871.327 ms\n",
      "epoch: 1327 train loss: 0.41509604 epoch time: 2821.182 ms\n",
      "epoch: 1328 train loss: 0.75612235 epoch time: 2725.908 ms\n",
      "epoch: 1329 train loss: 1.1306503 epoch time: 2866.248 ms\n",
      "epoch: 1330 train loss: 0.34883457 epoch time: 2947.738 ms\n",
      "epoch: 1331 train loss: 0.124012284 epoch time: 2683.208 ms\n",
      "epoch: 1332 train loss: 0.3611161 epoch time: 3005.658 ms\n",
      "epoch: 1333 train loss: 0.13024753 epoch time: 2789.702 ms\n",
      "epoch: 1334 train loss: 0.09676046 epoch time: 2495.779 ms\n",
      "epoch: 1335 train loss: 0.08929032 epoch time: 2583.323 ms\n",
      "epoch: 1336 train loss: 0.08664106 epoch time: 2436.575 ms\n",
      "epoch: 1337 train loss: 0.027135588 epoch time: 2487.044 ms\n",
      "epoch: 1338 train loss: 0.029880503 epoch time: 2516.721 ms\n",
      "epoch: 1339 train loss: 0.042743884 epoch time: 2542.583 ms\n",
      "epoch: 1340 train loss: 0.01233178 epoch time: 2505.997 ms\n",
      "epoch: 1341 train loss: 0.013394817 epoch time: 2771.009 ms\n",
      "epoch: 1342 train loss: 0.019521805 epoch time: 2890.472 ms\n",
      "epoch: 1343 train loss: 0.012253031 epoch time: 2907.930 ms\n",
      "epoch: 1344 train loss: 0.008828167 epoch time: 2810.068 ms\n",
      "epoch: 1345 train loss: 0.008944858 epoch time: 2828.326 ms\n",
      "epoch: 1346 train loss: 0.008176197 epoch time: 2773.138 ms\n",
      "epoch: 1347 train loss: 0.006697228 epoch time: 2822.544 ms\n",
      "epoch: 1348 train loss: 0.009584702 epoch time: 2651.554 ms\n",
      "epoch: 1349 train loss: 0.01265792 epoch time: 2675.654 ms\n",
      "epoch: 1350 train loss: 0.013990082 epoch time: 2714.068 ms\n",
      "epoch: 1351 train loss: 0.014045838 epoch time: 2686.200 ms\n",
      "epoch: 1352 train loss: 0.02672372 epoch time: 2775.107 ms\n",
      "epoch: 1353 train loss: 0.083697975 epoch time: 2880.805 ms\n",
      "epoch: 1354 train loss: 0.46240062 epoch time: 2808.465 ms\n",
      "epoch: 1355 train loss: 2.0397596 epoch time: 2646.217 ms\n",
      "epoch: 1356 train loss: 2.7022436 epoch time: 2536.249 ms\n",
      "epoch: 1357 train loss: 0.7855206 epoch time: 2632.281 ms\n",
      "epoch: 1358 train loss: 0.9632686 epoch time: 2492.117 ms\n",
      "epoch: 1359 train loss: 0.6266788 epoch time: 2658.566 ms\n",
      "epoch: 1360 train loss: 0.25938678 epoch time: 2756.828 ms\n",
      "epoch: 1361 train loss: 0.36079288 epoch time: 2953.409 ms\n",
      "epoch: 1362 train loss: 0.08755973 epoch time: 2808.365 ms\n",
      "epoch: 1363 train loss: 0.14339301 epoch time: 2707.594 ms\n",
      "epoch: 1364 train loss: 0.08265643 epoch time: 2769.252 ms\n",
      "epoch: 1365 train loss: 0.043868 epoch time: 2881.203 ms\n",
      "epoch: 1366 train loss: 0.049896404 epoch time: 2834.656 ms\n",
      "epoch: 1367 train loss: 0.030488053 epoch time: 2934.071 ms\n",
      "epoch: 1368 train loss: 0.028859403 epoch time: 2530.293 ms\n",
      "epoch: 1369 train loss: 0.0148048205 epoch time: 2733.662 ms\n",
      "epoch: 1370 train loss: 0.016256373 epoch time: 2943.283 ms\n",
      "epoch: 1371 train loss: 0.010269913 epoch time: 2801.223 ms\n",
      "epoch: 1372 train loss: 0.012197931 epoch time: 2970.250 ms\n",
      "epoch: 1373 train loss: 0.006829291 epoch time: 2783.878 ms\n",
      "epoch: 1374 train loss: 0.009122055 epoch time: 2818.772 ms\n",
      "epoch: 1375 train loss: 0.007254024 epoch time: 2836.857 ms\n",
      "epoch: 1376 train loss: 0.0071932697 epoch time: 2878.663 ms\n",
      "epoch: 1377 train loss: 0.006201967 epoch time: 2919.081 ms\n",
      "epoch: 1378 train loss: 0.006777198 epoch time: 2699.682 ms\n",
      "epoch: 1379 train loss: 0.0062416997 epoch time: 2948.946 ms\n",
      "epoch: 1380 train loss: 0.007569457 epoch time: 2788.552 ms\n",
      "epoch: 1381 train loss: 0.007286338 epoch time: 2812.712 ms\n",
      "epoch: 1382 train loss: 0.005999699 epoch time: 2612.009 ms\n",
      "epoch: 1383 train loss: 0.0070079886 epoch time: 2988.775 ms\n",
      "epoch: 1384 train loss: 0.006828491 epoch time: 2748.738 ms\n",
      "epoch: 1385 train loss: 0.0063587762 epoch time: 2611.991 ms\n",
      "epoch: 1386 train loss: 0.007333312 epoch time: 2844.950 ms\n",
      "epoch: 1387 train loss: 0.007852359 epoch time: 2904.571 ms\n",
      "epoch: 1388 train loss: 0.005845654 epoch time: 2731.537 ms\n",
      "epoch: 1389 train loss: 0.0060106223 epoch time: 2768.723 ms\n",
      "epoch: 1390 train loss: 0.0071328795 epoch time: 2743.693 ms\n",
      "epoch: 1391 train loss: 0.006324702 epoch time: 2936.152 ms\n",
      "epoch: 1392 train loss: 0.006333813 epoch time: 2760.133 ms\n",
      "epoch: 1393 train loss: 0.0067824624 epoch time: 2539.589 ms\n",
      "epoch: 1394 train loss: 0.006527442 epoch time: 2563.936 ms\n",
      "epoch: 1395 train loss: 0.0071036057 epoch time: 2516.437 ms\n",
      "epoch: 1396 train loss: 0.006728067 epoch time: 2651.608 ms\n",
      "epoch: 1397 train loss: 0.00649235 epoch time: 2879.174 ms\n",
      "epoch: 1398 train loss: 0.008863382 epoch time: 3064.422 ms\n",
      "epoch: 1399 train loss: 0.01813345 epoch time: 2783.224 ms\n",
      "epoch: 1400 train loss: 0.060290527 epoch time: 2977.615 ms\n",
      "    predict total time: 2.6636123657226562 ms\n",
      "    l2_error:  0.016374996341820387\n",
      "==================================================================================================\n",
      "epoch: 1401 train loss: 0.31087962 epoch time: 2706.245 ms\n",
      "epoch: 1402 train loss: 1.424587 epoch time: 2819.812 ms\n",
      "epoch: 1403 train loss: 2.339708 epoch time: 2683.434 ms\n",
      "epoch: 1404 train loss: 1.3199532 epoch time: 2779.036 ms\n",
      "epoch: 1405 train loss: 0.39834642 epoch time: 2787.214 ms\n",
      "epoch: 1406 train loss: 0.8095995 epoch time: 2867.405 ms\n",
      "epoch: 1407 train loss: 0.07429707 epoch time: 2728.460 ms\n",
      "epoch: 1408 train loss: 0.171998 epoch time: 2826.320 ms\n",
      "epoch: 1409 train loss: 0.21068121 epoch time: 2859.011 ms\n",
      "epoch: 1410 train loss: 0.093011655 epoch time: 2782.485 ms\n",
      "epoch: 1411 train loss: 0.017274424 epoch time: 2812.425 ms\n",
      "epoch: 1412 train loss: 0.052032094 epoch time: 2954.665 ms\n",
      "epoch: 1413 train loss: 0.03660509 epoch time: 2680.939 ms\n",
      "epoch: 1414 train loss: 0.009349867 epoch time: 2846.106 ms\n",
      "epoch: 1415 train loss: 0.01636068 epoch time: 2882.548 ms\n",
      "epoch: 1416 train loss: 0.014083979 epoch time: 2707.481 ms\n",
      "epoch: 1417 train loss: 0.009846259 epoch time: 2707.546 ms\n",
      "epoch: 1418 train loss: 0.008523496 epoch time: 2803.826 ms\n",
      "epoch: 1419 train loss: 0.009720117 epoch time: 2872.853 ms\n",
      "epoch: 1420 train loss: 0.006348358 epoch time: 2813.336 ms\n",
      "epoch: 1421 train loss: 0.006451616 epoch time: 2853.902 ms\n",
      "epoch: 1422 train loss: 0.0069858194 epoch time: 2862.959 ms\n",
      "epoch: 1423 train loss: 0.0063223587 epoch time: 2843.547 ms\n",
      "epoch: 1424 train loss: 0.0068569332 epoch time: 2776.489 ms\n",
      "epoch: 1425 train loss: 0.006127637 epoch time: 2841.220 ms\n",
      "epoch: 1426 train loss: 0.006266251 epoch time: 2840.005 ms\n",
      "epoch: 1427 train loss: 0.0069105076 epoch time: 2667.648 ms\n",
      "epoch: 1428 train loss: 0.0058927033 epoch time: 2847.913 ms\n",
      "epoch: 1429 train loss: 0.005968459 epoch time: 2769.862 ms\n",
      "epoch: 1430 train loss: 0.0056433924 epoch time: 2667.655 ms\n",
      "epoch: 1431 train loss: 0.007903144 epoch time: 2541.840 ms\n",
      "epoch: 1432 train loss: 0.0169821 epoch time: 2604.802 ms\n",
      "epoch: 1433 train loss: 0.058392443 epoch time: 2524.802 ms\n",
      "epoch: 1434 train loss: 0.23421131 epoch time: 2569.488 ms\n",
      "epoch: 1435 train loss: 0.9846643 epoch time: 2519.697 ms\n",
      "epoch: 1436 train loss: 2.6344266 epoch time: 2511.805 ms\n",
      "epoch: 1437 train loss: 0.72365224 epoch time: 2548.133 ms\n",
      "epoch: 1438 train loss: 0.7985627 epoch time: 2601.456 ms\n",
      "epoch: 1439 train loss: 0.31843916 epoch time: 2531.349 ms\n",
      "epoch: 1440 train loss: 0.38605797 epoch time: 2506.550 ms\n",
      "epoch: 1441 train loss: 0.12723002 epoch time: 2520.792 ms\n",
      "epoch: 1442 train loss: 0.1374647 epoch time: 2611.611 ms\n",
      "epoch: 1443 train loss: 0.11883131 epoch time: 2499.216 ms\n",
      "epoch: 1444 train loss: 0.04427433 epoch time: 2540.604 ms\n",
      "epoch: 1445 train loss: 0.08106244 epoch time: 2667.151 ms\n",
      "epoch: 1446 train loss: 0.046978224 epoch time: 2890.740 ms\n",
      "epoch: 1447 train loss: 0.020192344 epoch time: 2886.189 ms\n",
      "epoch: 1448 train loss: 0.038624696 epoch time: 2751.654 ms\n",
      "epoch: 1449 train loss: 0.019865556 epoch time: 2885.470 ms\n",
      "epoch: 1450 train loss: 0.012084649 epoch time: 2862.569 ms\n",
      "epoch: 1451 train loss: 0.014367305 epoch time: 2675.001 ms\n",
      "epoch: 1452 train loss: 0.006334508 epoch time: 2704.337 ms\n",
      "epoch: 1453 train loss: 0.010441983 epoch time: 2700.096 ms\n",
      "epoch: 1454 train loss: 0.006700888 epoch time: 2753.848 ms\n",
      "epoch: 1455 train loss: 0.0066818995 epoch time: 2677.314 ms\n",
      "epoch: 1456 train loss: 0.0061559374 epoch time: 2818.453 ms\n",
      "epoch: 1457 train loss: 0.005655924 epoch time: 2845.230 ms\n",
      "epoch: 1458 train loss: 0.005841204 epoch time: 2863.027 ms\n",
      "epoch: 1459 train loss: 0.0062847883 epoch time: 2870.209 ms\n",
      "epoch: 1460 train loss: 0.005992775 epoch time: 2911.415 ms\n",
      "epoch: 1461 train loss: 0.0059600207 epoch time: 2920.657 ms\n",
      "epoch: 1462 train loss: 0.0058790958 epoch time: 2587.626 ms\n",
      "epoch: 1463 train loss: 0.006227919 epoch time: 2997.899 ms\n",
      "epoch: 1464 train loss: 0.0060626944 epoch time: 2816.867 ms\n",
      "epoch: 1465 train loss: 0.006852214 epoch time: 2859.054 ms\n",
      "epoch: 1466 train loss: 0.0056058047 epoch time: 2693.842 ms\n",
      "epoch: 1467 train loss: 0.0052938 epoch time: 2476.942 ms\n",
      "epoch: 1468 train loss: 0.006903578 epoch time: 2489.496 ms\n",
      "epoch: 1469 train loss: 0.0063461377 epoch time: 2530.122 ms\n",
      "epoch: 1470 train loss: 0.010366278 epoch time: 2510.263 ms\n",
      "epoch: 1471 train loss: 0.026017856 epoch time: 2526.205 ms\n",
      "epoch: 1472 train loss: 0.0991876 epoch time: 2491.524 ms\n",
      "epoch: 1473 train loss: 0.459781 epoch time: 2508.920 ms\n",
      "epoch: 1474 train loss: 1.7174989 epoch time: 2561.842 ms\n",
      "epoch: 1475 train loss: 1.5596615 epoch time: 2462.763 ms\n",
      "epoch: 1476 train loss: 0.37438014 epoch time: 2549.336 ms\n",
      "epoch: 1477 train loss: 0.6888826 epoch time: 2846.436 ms\n",
      "epoch: 1478 train loss: 0.31611148 epoch time: 2884.307 ms\n",
      "epoch: 1479 train loss: 0.2126066 epoch time: 2974.035 ms\n",
      "epoch: 1480 train loss: 0.15692684 epoch time: 2913.729 ms\n",
      "epoch: 1481 train loss: 0.095012985 epoch time: 2687.611 ms\n",
      "epoch: 1482 train loss: 0.099684164 epoch time: 2531.817 ms\n",
      "epoch: 1483 train loss: 0.019518472 epoch time: 2488.509 ms\n",
      "epoch: 1484 train loss: 0.04890277 epoch time: 2537.185 ms\n",
      "epoch: 1485 train loss: 0.015080148 epoch time: 2559.674 ms\n",
      "epoch: 1486 train loss: 0.035224628 epoch time: 2678.984 ms\n",
      "epoch: 1487 train loss: 0.007278331 epoch time: 2802.175 ms\n",
      "epoch: 1488 train loss: 0.019063873 epoch time: 2785.867 ms\n",
      "epoch: 1489 train loss: 0.0127393305 epoch time: 2742.271 ms\n",
      "epoch: 1490 train loss: 0.008098108 epoch time: 2747.861 ms\n",
      "epoch: 1491 train loss: 0.009906749 epoch time: 2835.759 ms\n",
      "epoch: 1492 train loss: 0.00901704 epoch time: 2757.672 ms\n",
      "epoch: 1493 train loss: 0.0064170607 epoch time: 2568.969 ms\n",
      "epoch: 1494 train loss: 0.0064392816 epoch time: 2562.022 ms\n",
      "epoch: 1495 train loss: 0.006933771 epoch time: 2530.206 ms\n",
      "epoch: 1496 train loss: 0.006631489 epoch time: 2571.615 ms\n",
      "epoch: 1497 train loss: 0.0058873077 epoch time: 2483.840 ms\n",
      "epoch: 1498 train loss: 0.006114429 epoch time: 2517.726 ms\n",
      "epoch: 1499 train loss: 0.0053581116 epoch time: 2511.283 ms\n",
      "epoch: 1500 train loss: 0.0061067105 epoch time: 2576.574 ms\n",
      "    predict total time: 2.5081634521484375 ms\n",
      "    l2_error:  0.012506242528825297\n",
      "==================================================================================================\n",
      "epoch: 1501 train loss: 0.0060047456 epoch time: 2493.775 ms\n",
      "epoch: 1502 train loss: 0.005158507 epoch time: 2534.634 ms\n",
      "epoch: 1503 train loss: 0.007447662 epoch time: 2488.415 ms\n",
      "epoch: 1504 train loss: 0.008375096 epoch time: 2544.930 ms\n",
      "epoch: 1505 train loss: 0.008768929 epoch time: 2777.380 ms\n",
      "epoch: 1506 train loss: 0.012423996 epoch time: 2558.517 ms\n",
      "epoch: 1507 train loss: 0.032119956 epoch time: 2551.945 ms\n",
      "epoch: 1508 train loss: 0.1376464 epoch time: 2896.929 ms\n",
      "epoch: 1509 train loss: 0.60964286 epoch time: 2769.891 ms\n",
      "epoch: 1510 train loss: 2.3191576 epoch time: 2477.049 ms\n",
      "epoch: 1511 train loss: 1.7072413 epoch time: 2485.068 ms\n",
      "epoch: 1512 train loss: 0.31880197 epoch time: 2798.031 ms\n",
      "epoch: 1513 train loss: 0.76376075 epoch time: 2744.367 ms\n",
      "epoch: 1514 train loss: 0.19718546 epoch time: 2831.900 ms\n",
      "epoch: 1515 train loss: 0.24623974 epoch time: 2690.029 ms\n",
      "epoch: 1516 train loss: 0.22564569 epoch time: 2749.650 ms\n",
      "epoch: 1517 train loss: 0.04799307 epoch time: 2728.552 ms\n",
      "epoch: 1518 train loss: 0.17168576 epoch time: 2838.518 ms\n",
      "epoch: 1519 train loss: 0.011504114 epoch time: 2837.617 ms\n",
      "epoch: 1520 train loss: 0.07604713 epoch time: 2673.090 ms\n",
      "epoch: 1521 train loss: 0.016726295 epoch time: 2749.285 ms\n",
      "epoch: 1522 train loss: 0.040804274 epoch time: 2873.232 ms\n",
      "epoch: 1523 train loss: 0.021031622 epoch time: 3024.130 ms\n",
      "epoch: 1524 train loss: 0.017024545 epoch time: 2690.616 ms\n",
      "epoch: 1525 train loss: 0.011878926 epoch time: 2796.271 ms\n",
      "epoch: 1526 train loss: 0.010754311 epoch time: 2602.586 ms\n",
      "epoch: 1527 train loss: 0.010978147 epoch time: 2519.307 ms\n",
      "epoch: 1528 train loss: 0.011606032 epoch time: 2613.854 ms\n",
      "epoch: 1529 train loss: 0.014514005 epoch time: 2978.533 ms\n",
      "epoch: 1530 train loss: 0.03430178 epoch time: 2823.515 ms\n",
      "epoch: 1531 train loss: 0.07250614 epoch time: 2754.892 ms\n",
      "epoch: 1532 train loss: 0.18200375 epoch time: 2859.999 ms\n",
      "epoch: 1533 train loss: 0.40935007 epoch time: 2781.724 ms\n",
      "epoch: 1534 train loss: 0.33946222 epoch time: 2838.193 ms\n",
      "epoch: 1535 train loss: 0.032358244 epoch time: 2655.902 ms\n",
      "epoch: 1536 train loss: 0.09439342 epoch time: 2766.379 ms\n",
      "epoch: 1537 train loss: 0.12488424 epoch time: 2877.385 ms\n",
      "epoch: 1538 train loss: 0.027475713 epoch time: 2898.575 ms\n",
      "epoch: 1539 train loss: 0.04253028 epoch time: 2918.058 ms\n",
      "epoch: 1540 train loss: 0.044319797 epoch time: 2750.840 ms\n",
      "epoch: 1541 train loss: 0.017826667 epoch time: 2712.585 ms\n",
      "epoch: 1542 train loss: 0.031872313 epoch time: 2816.241 ms\n",
      "epoch: 1543 train loss: 0.01900878 epoch time: 2730.354 ms\n",
      "epoch: 1544 train loss: 0.012026523 epoch time: 2586.635 ms\n",
      "epoch: 1545 train loss: 0.015648812 epoch time: 2527.401 ms\n",
      "epoch: 1546 train loss: 0.009069881 epoch time: 2692.870 ms\n",
      "epoch: 1547 train loss: 0.007542973 epoch time: 2772.806 ms\n",
      "epoch: 1548 train loss: 0.013914973 epoch time: 2570.715 ms\n",
      "epoch: 1549 train loss: 0.016458021 epoch time: 2480.603 ms\n",
      "epoch: 1550 train loss: 0.020666808 epoch time: 2455.515 ms\n",
      "epoch: 1551 train loss: 0.03367351 epoch time: 2540.538 ms\n",
      "epoch: 1552 train loss: 0.07358465 epoch time: 2566.286 ms\n",
      "epoch: 1553 train loss: 0.19670881 epoch time: 2837.044 ms\n",
      "epoch: 1554 train loss: 0.5341963 epoch time: 2784.506 ms\n",
      "epoch: 1555 train loss: 1.0953112 epoch time: 2850.732 ms\n",
      "epoch: 1556 train loss: 0.8923173 epoch time: 2794.005 ms\n",
      "epoch: 1557 train loss: 0.08800832 epoch time: 2827.956 ms\n",
      "epoch: 1558 train loss: 0.5079961 epoch time: 2814.925 ms\n",
      "epoch: 1559 train loss: 0.088698514 epoch time: 2840.066 ms\n",
      "epoch: 1560 train loss: 0.14674272 epoch time: 3030.185 ms\n",
      "epoch: 1561 train loss: 0.13577154 epoch time: 2798.563 ms\n",
      "epoch: 1562 train loss: 0.018536072 epoch time: 2889.988 ms\n",
      "epoch: 1563 train loss: 0.10067574 epoch time: 2731.915 ms\n",
      "epoch: 1564 train loss: 0.037043065 epoch time: 2887.189 ms\n",
      "epoch: 1565 train loss: 0.046500396 epoch time: 2693.193 ms\n",
      "epoch: 1566 train loss: 0.07233895 epoch time: 2589.105 ms\n",
      "epoch: 1567 train loss: 0.04702508 epoch time: 2777.889 ms\n",
      "epoch: 1568 train loss: 0.067244574 epoch time: 2562.227 ms\n",
      "epoch: 1569 train loss: 0.097440444 epoch time: 2503.387 ms\n",
      "epoch: 1570 train loss: 0.12534533 epoch time: 2643.775 ms\n",
      "epoch: 1571 train loss: 0.116495565 epoch time: 2550.378 ms\n",
      "epoch: 1572 train loss: 0.10647247 epoch time: 2589.227 ms\n",
      "epoch: 1573 train loss: 0.091238715 epoch time: 2489.083 ms\n",
      "epoch: 1574 train loss: 0.076370426 epoch time: 2593.650 ms\n",
      "epoch: 1575 train loss: 0.076435186 epoch time: 2570.841 ms\n",
      "epoch: 1576 train loss: 0.13108627 epoch time: 2503.822 ms\n",
      "epoch: 1577 train loss: 0.3399671 epoch time: 2759.779 ms\n",
      "epoch: 1578 train loss: 0.70318645 epoch time: 2912.801 ms\n",
      "epoch: 1579 train loss: 0.57709295 epoch time: 2999.254 ms\n",
      "epoch: 1580 train loss: 0.13810258 epoch time: 2819.913 ms\n",
      "epoch: 1581 train loss: 0.113313325 epoch time: 2941.793 ms\n",
      "epoch: 1582 train loss: 0.07474619 epoch time: 2490.667 ms\n",
      "epoch: 1583 train loss: 0.105032526 epoch time: 2524.312 ms\n",
      "epoch: 1584 train loss: 0.065265104 epoch time: 2505.040 ms\n",
      "epoch: 1585 train loss: 0.009691419 epoch time: 2710.306 ms\n",
      "epoch: 1586 train loss: 0.016855538 epoch time: 2771.449 ms\n",
      "epoch: 1587 train loss: 0.036286034 epoch time: 2773.357 ms\n",
      "epoch: 1588 train loss: 0.043253176 epoch time: 2717.919 ms\n",
      "epoch: 1589 train loss: 0.062148675 epoch time: 2786.767 ms\n",
      "epoch: 1590 train loss: 0.06475393 epoch time: 2870.704 ms\n",
      "epoch: 1591 train loss: 0.07797095 epoch time: 2554.168 ms\n",
      "epoch: 1592 train loss: 0.10814492 epoch time: 2851.183 ms\n",
      "epoch: 1593 train loss: 0.1766617 epoch time: 2928.554 ms\n",
      "epoch: 1594 train loss: 0.33148733 epoch time: 2845.169 ms\n",
      "epoch: 1595 train loss: 0.48402208 epoch time: 2942.105 ms\n",
      "epoch: 1596 train loss: 0.5062376 epoch time: 2867.576 ms\n",
      "epoch: 1597 train loss: 0.12984566 epoch time: 2773.457 ms\n",
      "epoch: 1598 train loss: 0.016282145 epoch time: 2975.007 ms\n",
      "epoch: 1599 train loss: 0.12547088 epoch time: 2594.749 ms\n",
      "epoch: 1600 train loss: 0.12792452 epoch time: 2496.096 ms\n",
      "    predict total time: 2.577543258666992 ms\n",
      "    l2_error:  0.019015456369597814\n",
      "==================================================================================================\n",
      "epoch: 1601 train loss: 0.043137975 epoch time: 2493.708 ms\n",
      "epoch: 1602 train loss: 0.006690806 epoch time: 2658.431 ms\n",
      "epoch: 1603 train loss: 0.025048748 epoch time: 2537.316 ms\n",
      "epoch: 1604 train loss: 0.042342503 epoch time: 2682.087 ms\n",
      "epoch: 1605 train loss: 0.05158295 epoch time: 2934.284 ms\n",
      "epoch: 1606 train loss: 0.05450499 epoch time: 2866.258 ms\n",
      "epoch: 1607 train loss: 0.06924217 epoch time: 2913.252 ms\n",
      "epoch: 1608 train loss: 0.15273024 epoch time: 2854.663 ms\n",
      "epoch: 1609 train loss: 0.46866113 epoch time: 2745.402 ms\n",
      "epoch: 1610 train loss: 0.7993137 epoch time: 2912.117 ms\n",
      "epoch: 1611 train loss: 0.15642735 epoch time: 2747.103 ms\n",
      "epoch: 1612 train loss: 0.22401893 epoch time: 2877.852 ms\n",
      "epoch: 1613 train loss: 0.1471624 epoch time: 2846.029 ms\n",
      "epoch: 1614 train loss: 0.08317059 epoch time: 2855.140 ms\n",
      "epoch: 1615 train loss: 0.087899156 epoch time: 2757.209 ms\n",
      "epoch: 1616 train loss: 0.066058844 epoch time: 2762.370 ms\n",
      "epoch: 1617 train loss: 0.12487683 epoch time: 2923.522 ms\n",
      "epoch: 1618 train loss: 0.25172493 epoch time: 2853.727 ms\n",
      "epoch: 1619 train loss: 0.7089216 epoch time: 2862.324 ms\n",
      "epoch: 1620 train loss: 1.2642376 epoch time: 2771.260 ms\n",
      "epoch: 1621 train loss: 0.5640667 epoch time: 2836.897 ms\n",
      "epoch: 1622 train loss: 0.03118853 epoch time: 2927.430 ms\n",
      "epoch: 1623 train loss: 0.3158665 epoch time: 2729.275 ms\n",
      "epoch: 1624 train loss: 0.058405854 epoch time: 2851.380 ms\n",
      "epoch: 1625 train loss: 0.055797502 epoch time: 2917.910 ms\n",
      "epoch: 1626 train loss: 0.092075534 epoch time: 2846.397 ms\n",
      "epoch: 1627 train loss: 0.01137735 epoch time: 2872.281 ms\n",
      "epoch: 1628 train loss: 0.032896243 epoch time: 2907.357 ms\n",
      "epoch: 1629 train loss: 0.029691134 epoch time: 2941.888 ms\n",
      "epoch: 1630 train loss: 0.0077078077 epoch time: 2917.202 ms\n",
      "epoch: 1631 train loss: 0.011431871 epoch time: 2722.680 ms\n",
      "epoch: 1632 train loss: 0.016171059 epoch time: 2867.906 ms\n",
      "epoch: 1633 train loss: 0.011031464 epoch time: 2898.155 ms\n",
      "epoch: 1634 train loss: 0.005595871 epoch time: 2831.716 ms\n",
      "epoch: 1635 train loss: 0.0073906644 epoch time: 2758.317 ms\n",
      "epoch: 1636 train loss: 0.012959892 epoch time: 2694.834 ms\n",
      "epoch: 1637 train loss: 0.014624744 epoch time: 2804.980 ms\n",
      "epoch: 1638 train loss: 0.015939925 epoch time: 2885.438 ms\n",
      "epoch: 1639 train loss: 0.020594 epoch time: 2751.677 ms\n",
      "epoch: 1640 train loss: 0.024222653 epoch time: 2774.512 ms\n",
      "epoch: 1641 train loss: 0.034015246 epoch time: 2748.449 ms\n",
      "epoch: 1642 train loss: 0.07612283 epoch time: 2808.355 ms\n",
      "epoch: 1643 train loss: 0.16727394 epoch time: 2803.645 ms\n",
      "epoch: 1644 train loss: 0.43259948 epoch time: 2679.250 ms\n",
      "epoch: 1645 train loss: 1.0562919 epoch time: 2530.957 ms\n",
      "epoch: 1646 train loss: 1.4519385 epoch time: 2621.165 ms\n",
      "epoch: 1647 train loss: 0.16955556 epoch time: 2718.780 ms\n",
      "epoch: 1648 train loss: 0.4317638 epoch time: 2879.324 ms\n",
      "epoch: 1649 train loss: 0.16988184 epoch time: 2707.213 ms\n",
      "epoch: 1650 train loss: 0.20182738 epoch time: 2811.050 ms\n",
      "epoch: 1651 train loss: 0.12241367 epoch time: 2734.497 ms\n",
      "epoch: 1652 train loss: 0.080967054 epoch time: 2869.074 ms\n",
      "epoch: 1653 train loss: 0.066234544 epoch time: 2758.187 ms\n",
      "epoch: 1654 train loss: 0.026970487 epoch time: 2666.960 ms\n",
      "epoch: 1655 train loss: 0.073965214 epoch time: 2910.294 ms\n",
      "epoch: 1656 train loss: 0.016905362 epoch time: 2717.527 ms\n",
      "epoch: 1657 train loss: 0.036877457 epoch time: 2643.563 ms\n",
      "epoch: 1658 train loss: 0.01915927 epoch time: 2472.070 ms\n",
      "epoch: 1659 train loss: 0.015233122 epoch time: 2578.687 ms\n",
      "epoch: 1660 train loss: 0.020937683 epoch time: 2945.079 ms\n",
      "epoch: 1661 train loss: 0.010770983 epoch time: 2999.152 ms\n",
      "epoch: 1662 train loss: 0.016056651 epoch time: 2855.540 ms\n",
      "epoch: 1663 train loss: 0.02648544 epoch time: 2825.635 ms\n",
      "epoch: 1664 train loss: 0.04646925 epoch time: 2722.759 ms\n",
      "epoch: 1665 train loss: 0.08894516 epoch time: 2719.246 ms\n",
      "epoch: 1666 train loss: 0.2374377 epoch time: 2702.357 ms\n",
      "epoch: 1667 train loss: 0.50968045 epoch time: 2795.807 ms\n",
      "epoch: 1668 train loss: 0.19325241 epoch time: 2851.617 ms\n",
      "epoch: 1669 train loss: 0.028400835 epoch time: 2810.789 ms\n",
      "epoch: 1670 train loss: 0.16554032 epoch time: 2543.984 ms\n",
      "epoch: 1671 train loss: 0.057407137 epoch time: 2537.983 ms\n",
      "epoch: 1672 train loss: 0.04450046 epoch time: 2836.651 ms\n",
      "epoch: 1673 train loss: 0.06262719 epoch time: 2473.948 ms\n",
      "epoch: 1674 train loss: 0.011092936 epoch time: 2518.074 ms\n",
      "epoch: 1675 train loss: 0.016100181 epoch time: 2849.049 ms\n",
      "epoch: 1676 train loss: 0.021147352 epoch time: 2732.803 ms\n",
      "epoch: 1677 train loss: 0.009950928 epoch time: 3034.650 ms\n",
      "epoch: 1678 train loss: 0.017343475 epoch time: 2920.043 ms\n",
      "epoch: 1679 train loss: 0.044570863 epoch time: 2711.873 ms\n",
      "epoch: 1680 train loss: 0.13161732 epoch time: 2805.426 ms\n",
      "epoch: 1681 train loss: 0.6322307 epoch time: 2567.186 ms\n",
      "epoch: 1682 train loss: 2.1588824 epoch time: 2794.804 ms\n",
      "epoch: 1683 train loss: 1.1741877 epoch time: 2904.042 ms\n",
      "epoch: 1684 train loss: 0.3620191 epoch time: 2792.131 ms\n",
      "epoch: 1685 train loss: 0.5409841 epoch time: 2678.066 ms\n",
      "epoch: 1686 train loss: 0.19002017 epoch time: 2812.086 ms\n",
      "epoch: 1687 train loss: 0.11176075 epoch time: 2926.569 ms\n",
      "epoch: 1688 train loss: 0.18824205 epoch time: 2857.303 ms\n",
      "epoch: 1689 train loss: 0.041224595 epoch time: 2892.256 ms\n",
      "epoch: 1690 train loss: 0.12841408 epoch time: 2941.504 ms\n",
      "epoch: 1691 train loss: 0.011489145 epoch time: 2673.234 ms\n",
      "epoch: 1692 train loss: 0.06607779 epoch time: 2753.054 ms\n",
      "epoch: 1693 train loss: 0.013836858 epoch time: 2803.049 ms\n",
      "epoch: 1694 train loss: 0.034492478 epoch time: 2846.492 ms\n",
      "epoch: 1695 train loss: 0.007095079 epoch time: 2842.401 ms\n",
      "epoch: 1696 train loss: 0.01946668 epoch time: 2773.998 ms\n",
      "epoch: 1697 train loss: 0.008008326 epoch time: 2916.628 ms\n",
      "epoch: 1698 train loss: 0.012501695 epoch time: 2663.309 ms\n",
      "epoch: 1699 train loss: 0.006720673 epoch time: 2761.114 ms\n",
      "epoch: 1700 train loss: 0.011968375 epoch time: 2961.083 ms\n",
      "    predict total time: 2.6547908782958984 ms\n",
      "    l2_error:  0.012869372019634194\n",
      "==================================================================================================\n",
      "epoch: 1701 train loss: 0.020810606 epoch time: 2683.223 ms\n",
      "epoch: 1702 train loss: 0.034660604 epoch time: 2613.055 ms\n",
      "epoch: 1703 train loss: 0.060906734 epoch time: 2857.920 ms\n",
      "epoch: 1704 train loss: 0.08673671 epoch time: 2789.186 ms\n",
      "epoch: 1705 train loss: 0.0945841 epoch time: 2931.443 ms\n",
      "epoch: 1706 train loss: 0.10691604 epoch time: 2868.967 ms\n",
      "epoch: 1707 train loss: 0.12963553 epoch time: 2688.327 ms\n",
      "epoch: 1708 train loss: 0.1510565 epoch time: 2610.534 ms\n",
      "epoch: 1709 train loss: 0.11161933 epoch time: 2486.356 ms\n",
      "epoch: 1710 train loss: 0.04935145 epoch time: 2518.910 ms\n",
      "epoch: 1711 train loss: 0.014354907 epoch time: 2513.889 ms\n",
      "epoch: 1712 train loss: 0.009373354 epoch time: 2496.976 ms\n",
      "epoch: 1713 train loss: 0.011639939 epoch time: 2538.072 ms\n",
      "epoch: 1714 train loss: 0.01600601 epoch time: 2827.348 ms\n",
      "epoch: 1715 train loss: 0.028175956 epoch time: 2875.523 ms\n",
      "epoch: 1716 train loss: 0.05206247 epoch time: 2606.827 ms\n",
      "epoch: 1717 train loss: 0.11033499 epoch time: 2854.748 ms\n",
      "epoch: 1718 train loss: 0.23994382 epoch time: 3027.749 ms\n",
      "epoch: 1719 train loss: 0.49013704 epoch time: 2869.092 ms\n",
      "epoch: 1720 train loss: 0.45805308 epoch time: 2691.393 ms\n",
      "epoch: 1721 train loss: 0.13178428 epoch time: 2956.395 ms\n",
      "epoch: 1722 train loss: 0.1241867 epoch time: 2935.080 ms\n",
      "epoch: 1723 train loss: 0.1417636 epoch time: 2749.895 ms\n",
      "epoch: 1724 train loss: 0.043356474 epoch time: 2823.743 ms\n",
      "epoch: 1725 train loss: 0.029768033 epoch time: 2904.508 ms\n",
      "epoch: 1726 train loss: 0.06796362 epoch time: 2885.414 ms\n",
      "epoch: 1727 train loss: 0.06926942 epoch time: 2691.482 ms\n",
      "epoch: 1728 train loss: 0.09219512 epoch time: 2731.541 ms\n",
      "epoch: 1729 train loss: 0.16070686 epoch time: 2596.972 ms\n",
      "epoch: 1730 train loss: 0.2729955 epoch time: 2469.302 ms\n",
      "epoch: 1731 train loss: 0.52243537 epoch time: 2522.818 ms\n",
      "epoch: 1732 train loss: 0.7109943 epoch time: 2489.866 ms\n",
      "epoch: 1733 train loss: 0.2853719 epoch time: 2529.500 ms\n",
      "epoch: 1734 train loss: 0.01346007 epoch time: 2780.417 ms\n",
      "epoch: 1735 train loss: 0.15066345 epoch time: 2525.653 ms\n",
      "epoch: 1736 train loss: 0.15911403 epoch time: 2474.866 ms\n",
      "epoch: 1737 train loss: 0.051780406 epoch time: 2526.916 ms\n",
      "epoch: 1738 train loss: 0.014782816 epoch time: 2492.537 ms\n",
      "epoch: 1739 train loss: 0.053770386 epoch time: 2618.579 ms\n",
      "epoch: 1740 train loss: 0.055661794 epoch time: 2601.471 ms\n",
      "epoch: 1741 train loss: 0.023007235 epoch time: 2943.706 ms\n",
      "epoch: 1742 train loss: 0.010592986 epoch time: 2510.556 ms\n",
      "epoch: 1743 train loss: 0.021493575 epoch time: 2782.841 ms\n",
      "epoch: 1744 train loss: 0.035802264 epoch time: 2847.980 ms\n",
      "epoch: 1745 train loss: 0.050990935 epoch time: 2781.931 ms\n",
      "epoch: 1746 train loss: 0.17634557 epoch time: 2704.694 ms\n",
      "epoch: 1747 train loss: 0.36358157 epoch time: 2807.536 ms\n",
      "epoch: 1748 train loss: 0.24220951 epoch time: 2788.493 ms\n",
      "epoch: 1749 train loss: 0.086551055 epoch time: 2838.179 ms\n",
      "epoch: 1750 train loss: 0.16682123 epoch time: 2711.517 ms\n",
      "epoch: 1751 train loss: 0.38704437 epoch time: 2542.720 ms\n",
      "epoch: 1752 train loss: 0.6241652 epoch time: 2510.729 ms\n",
      "epoch: 1753 train loss: 0.6581274 epoch time: 2504.464 ms\n",
      "epoch: 1754 train loss: 0.24307539 epoch time: 2622.546 ms\n",
      "epoch: 1755 train loss: 0.010763446 epoch time: 2768.451 ms\n",
      "epoch: 1756 train loss: 0.13661988 epoch time: 2838.722 ms\n",
      "epoch: 1757 train loss: 0.13772754 epoch time: 2715.803 ms\n",
      "epoch: 1758 train loss: 0.027049787 epoch time: 2666.513 ms\n",
      "epoch: 1759 train loss: 0.01661216 epoch time: 2830.001 ms\n",
      "epoch: 1760 train loss: 0.046372954 epoch time: 2746.703 ms\n",
      "epoch: 1761 train loss: 0.04690717 epoch time: 2849.083 ms\n",
      "epoch: 1762 train loss: 0.026901964 epoch time: 2916.513 ms\n",
      "epoch: 1763 train loss: 0.010915032 epoch time: 2712.648 ms\n",
      "epoch: 1764 train loss: 0.009353975 epoch time: 2667.983 ms\n",
      "epoch: 1765 train loss: 0.015878594 epoch time: 2755.048 ms\n",
      "epoch: 1766 train loss: 0.021229578 epoch time: 2514.471 ms\n",
      "epoch: 1767 train loss: 0.03231303 epoch time: 2701.077 ms\n",
      "epoch: 1768 train loss: 0.044804078 epoch time: 2741.421 ms\n",
      "epoch: 1769 train loss: 0.050301462 epoch time: 2845.396 ms\n",
      "epoch: 1770 train loss: 0.09131726 epoch time: 2892.852 ms\n",
      "epoch: 1771 train loss: 0.17167293 epoch time: 2934.908 ms\n",
      "epoch: 1772 train loss: 0.34081653 epoch time: 2711.082 ms\n",
      "epoch: 1773 train loss: 0.5294136 epoch time: 2794.160 ms\n",
      "epoch: 1774 train loss: 0.507794 epoch time: 2902.466 ms\n",
      "epoch: 1775 train loss: 0.4812404 epoch time: 2843.528 ms\n",
      "epoch: 1776 train loss: 0.82138234 epoch time: 2741.393 ms\n",
      "epoch: 1777 train loss: 0.40841615 epoch time: 2612.483 ms\n",
      "epoch: 1778 train loss: 0.17142001 epoch time: 2544.634 ms\n",
      "epoch: 1779 train loss: 0.18032491 epoch time: 2908.882 ms\n",
      "epoch: 1780 train loss: 0.17540233 epoch time: 2946.642 ms\n",
      "epoch: 1781 train loss: 0.075076416 epoch time: 2850.336 ms\n",
      "epoch: 1782 train loss: 0.041408747 epoch time: 2839.341 ms\n",
      "epoch: 1783 train loss: 0.050253518 epoch time: 2901.962 ms\n",
      "epoch: 1784 train loss: 0.04737716 epoch time: 2754.604 ms\n",
      "epoch: 1785 train loss: 0.032503545 epoch time: 2787.935 ms\n",
      "epoch: 1786 train loss: 0.01185371 epoch time: 2820.002 ms\n",
      "epoch: 1787 train loss: 0.017553655 epoch time: 2774.410 ms\n",
      "epoch: 1788 train loss: 0.016738154 epoch time: 2574.512 ms\n",
      "epoch: 1789 train loss: 0.05415589 epoch time: 2629.050 ms\n",
      "epoch: 1790 train loss: 0.11384021 epoch time: 2842.627 ms\n",
      "epoch: 1791 train loss: 0.18552254 epoch time: 2888.945 ms\n",
      "epoch: 1792 train loss: 0.2886657 epoch time: 2865.183 ms\n",
      "epoch: 1793 train loss: 0.35743085 epoch time: 2873.477 ms\n",
      "epoch: 1794 train loss: 0.3369316 epoch time: 2862.657 ms\n",
      "epoch: 1795 train loss: 0.25578532 epoch time: 2790.830 ms\n",
      "epoch: 1796 train loss: 0.083530225 epoch time: 2872.381 ms\n",
      "epoch: 1797 train loss: 0.025034772 epoch time: 2762.499 ms\n",
      "epoch: 1798 train loss: 0.023807302 epoch time: 2839.709 ms\n",
      "epoch: 1799 train loss: 0.049507406 epoch time: 2811.822 ms\n",
      "epoch: 1800 train loss: 0.06195729 epoch time: 2680.360 ms\n",
      "    predict total time: 2.530813217163086 ms\n",
      "    l2_error:  0.014935816315034864\n",
      "==================================================================================================\n",
      "epoch: 1801 train loss: 0.06196973 epoch time: 2707.476 ms\n",
      "epoch: 1802 train loss: 0.047758713 epoch time: 2838.941 ms\n",
      "epoch: 1803 train loss: 0.04117981 epoch time: 2706.484 ms\n",
      "epoch: 1804 train loss: 0.04202924 epoch time: 2730.391 ms\n",
      "epoch: 1805 train loss: 0.07027432 epoch time: 2769.712 ms\n",
      "epoch: 1806 train loss: 0.14040305 epoch time: 2872.231 ms\n",
      "epoch: 1807 train loss: 0.30785942 epoch time: 2795.526 ms\n",
      "epoch: 1808 train loss: 0.5086445 epoch time: 2572.258 ms\n",
      "epoch: 1809 train loss: 0.5936593 epoch time: 2570.054 ms\n",
      "epoch: 1810 train loss: 0.26924962 epoch time: 2581.204 ms\n",
      "epoch: 1811 train loss: 0.012803432 epoch time: 2496.302 ms\n",
      "epoch: 1812 train loss: 0.12607569 epoch time: 2496.247 ms\n",
      "epoch: 1813 train loss: 0.13202234 epoch time: 2584.387 ms\n",
      "epoch: 1814 train loss: 0.043082274 epoch time: 2930.876 ms\n",
      "epoch: 1815 train loss: 0.008979568 epoch time: 2671.780 ms\n",
      "epoch: 1816 train loss: 0.01589096 epoch time: 2673.674 ms\n",
      "epoch: 1817 train loss: 0.026214004 epoch time: 2528.157 ms\n",
      "epoch: 1818 train loss: 0.032290526 epoch time: 2520.319 ms\n",
      "epoch: 1819 train loss: 0.04354709 epoch time: 2470.528 ms\n",
      "epoch: 1820 train loss: 0.097247906 epoch time: 2761.552 ms\n",
      "epoch: 1821 train loss: 0.26138675 epoch time: 2831.722 ms\n",
      "epoch: 1822 train loss: 0.44374898 epoch time: 2672.956 ms\n",
      "epoch: 1823 train loss: 0.45116743 epoch time: 2886.233 ms\n",
      "epoch: 1824 train loss: 0.21005595 epoch time: 2904.820 ms\n",
      "epoch: 1825 train loss: 0.2023562 epoch time: 2945.815 ms\n",
      "epoch: 1826 train loss: 0.16090012 epoch time: 2585.158 ms\n",
      "epoch: 1827 train loss: 0.045495138 epoch time: 2479.726 ms\n",
      "epoch: 1828 train loss: 0.063120864 epoch time: 2573.793 ms\n",
      "epoch: 1829 train loss: 0.02641304 epoch time: 2692.730 ms\n",
      "epoch: 1830 train loss: 0.009881701 epoch time: 2693.325 ms\n",
      "epoch: 1831 train loss: 0.025193162 epoch time: 2668.471 ms\n",
      "epoch: 1832 train loss: 0.01710305 epoch time: 2850.090 ms\n",
      "epoch: 1833 train loss: 0.035279226 epoch time: 2867.625 ms\n",
      "epoch: 1834 train loss: 0.113039844 epoch time: 2650.283 ms\n",
      "epoch: 1835 train loss: 0.3253767 epoch time: 2934.774 ms\n",
      "epoch: 1836 train loss: 1.0169647 epoch time: 2678.669 ms\n",
      "epoch: 1837 train loss: 1.4134204 epoch time: 2877.532 ms\n",
      "epoch: 1838 train loss: 0.31469268 epoch time: 2826.734 ms\n",
      "epoch: 1839 train loss: 0.30363545 epoch time: 2725.727 ms\n",
      "epoch: 1840 train loss: 0.3387209 epoch time: 2862.485 ms\n",
      "epoch: 1841 train loss: 0.10809133 epoch time: 2623.655 ms\n",
      "epoch: 1842 train loss: 0.19671644 epoch time: 2937.131 ms\n",
      "epoch: 1843 train loss: 0.058062606 epoch time: 2578.844 ms\n",
      "epoch: 1844 train loss: 0.11537571 epoch time: 2731.667 ms\n",
      "epoch: 1845 train loss: 0.026881412 epoch time: 2901.869 ms\n",
      "epoch: 1846 train loss: 0.044229865 epoch time: 2906.521 ms\n",
      "epoch: 1847 train loss: 0.026695177 epoch time: 2834.185 ms\n",
      "epoch: 1848 train loss: 0.007598385 epoch time: 2532.303 ms\n",
      "epoch: 1849 train loss: 0.022245321 epoch time: 2500.659 ms\n",
      "epoch: 1850 train loss: 0.008899555 epoch time: 2692.321 ms\n",
      "epoch: 1851 train loss: 0.007183801 epoch time: 2512.463 ms\n",
      "epoch: 1852 train loss: 0.013815581 epoch time: 2507.592 ms\n",
      "epoch: 1853 train loss: 0.00979261 epoch time: 2584.361 ms\n",
      "epoch: 1854 train loss: 0.0049909605 epoch time: 2513.853 ms\n",
      "epoch: 1855 train loss: 0.006767898 epoch time: 2596.081 ms\n",
      "epoch: 1856 train loss: 0.0071370075 epoch time: 2505.132 ms\n",
      "epoch: 1857 train loss: 0.0069267387 epoch time: 2494.418 ms\n",
      "epoch: 1858 train loss: 0.0060217963 epoch time: 2795.141 ms\n",
      "epoch: 1859 train loss: 0.005389101 epoch time: 3012.712 ms\n",
      "epoch: 1860 train loss: 0.007406931 epoch time: 2925.406 ms\n",
      "epoch: 1861 train loss: 0.013909625 epoch time: 2745.759 ms\n",
      "epoch: 1862 train loss: 0.03431798 epoch time: 2869.421 ms\n",
      "epoch: 1863 train loss: 0.1030713 epoch time: 3076.858 ms\n",
      "epoch: 1864 train loss: 0.35953552 epoch time: 2831.944 ms\n",
      "epoch: 1865 train loss: 0.8665354 epoch time: 2687.433 ms\n",
      "epoch: 1866 train loss: 0.09795099 epoch time: 2654.340 ms\n",
      "epoch: 1867 train loss: 0.34553045 epoch time: 2837.272 ms\n",
      "epoch: 1868 train loss: 0.22930704 epoch time: 2703.713 ms\n",
      "epoch: 1869 train loss: 0.68801874 epoch time: 2774.326 ms\n",
      "epoch: 1870 train loss: 0.8665226 epoch time: 2678.618 ms\n",
      "epoch: 1871 train loss: 0.92278534 epoch time: 2881.681 ms\n",
      "epoch: 1872 train loss: 0.081131555 epoch time: 2828.716 ms\n",
      "epoch: 1873 train loss: 0.27818993 epoch time: 2702.710 ms\n",
      "epoch: 1874 train loss: 0.20089181 epoch time: 2770.687 ms\n",
      "epoch: 1875 train loss: 0.016861543 epoch time: 2897.884 ms\n",
      "epoch: 1876 train loss: 0.10390742 epoch time: 2843.662 ms\n",
      "epoch: 1877 train loss: 0.037812375 epoch time: 2732.131 ms\n",
      "epoch: 1878 train loss: 0.017153455 epoch time: 2773.702 ms\n",
      "epoch: 1879 train loss: 0.037888166 epoch time: 2905.190 ms\n",
      "epoch: 1880 train loss: 0.015322811 epoch time: 2825.448 ms\n",
      "epoch: 1881 train loss: 0.008048811 epoch time: 2647.661 ms\n",
      "epoch: 1882 train loss: 0.012353481 epoch time: 2491.148 ms\n",
      "epoch: 1883 train loss: 0.008598452 epoch time: 2573.764 ms\n",
      "epoch: 1884 train loss: 0.00450589 epoch time: 2646.630 ms\n",
      "epoch: 1885 train loss: 0.0058044875 epoch time: 2695.814 ms\n",
      "epoch: 1886 train loss: 0.008367304 epoch time: 2740.489 ms\n",
      "epoch: 1887 train loss: 0.011067202 epoch time: 2840.165 ms\n",
      "epoch: 1888 train loss: 0.012735337 epoch time: 2679.352 ms\n",
      "epoch: 1889 train loss: 0.014161165 epoch time: 2736.020 ms\n",
      "epoch: 1890 train loss: 0.01206441 epoch time: 2679.507 ms\n",
      "epoch: 1891 train loss: 0.0139400065 epoch time: 2896.834 ms\n",
      "epoch: 1892 train loss: 0.021408504 epoch time: 2878.024 ms\n",
      "epoch: 1893 train loss: 0.046342127 epoch time: 2816.463 ms\n",
      "epoch: 1894 train loss: 0.1034845 epoch time: 2593.354 ms\n",
      "epoch: 1895 train loss: 0.3086586 epoch time: 2523.642 ms\n",
      "epoch: 1896 train loss: 0.90908813 epoch time: 2478.493 ms\n",
      "epoch: 1897 train loss: 1.4230996 epoch time: 2484.980 ms\n",
      "epoch: 1898 train loss: 0.08099337 epoch time: 2668.081 ms\n",
      "epoch: 1899 train loss: 0.48125896 epoch time: 2658.513 ms\n",
      "epoch: 1900 train loss: 0.27173275 epoch time: 2926.403 ms\n",
      "    predict total time: 2.651214599609375 ms\n",
      "    l2_error:  0.01741804952314982\n",
      "==================================================================================================\n",
      "epoch: 1901 train loss: 0.07147219 epoch time: 3023.482 ms\n",
      "epoch: 1902 train loss: 0.15840185 epoch time: 2989.006 ms\n",
      "epoch: 1903 train loss: 0.016808093 epoch time: 2516.641 ms\n",
      "epoch: 1904 train loss: 0.075654685 epoch time: 2458.651 ms\n",
      "epoch: 1905 train loss: 0.018456314 epoch time: 2565.720 ms\n",
      "epoch: 1906 train loss: 0.044568498 epoch time: 2641.222 ms\n",
      "epoch: 1907 train loss: 0.013151005 epoch time: 2744.613 ms\n",
      "epoch: 1908 train loss: 0.01903678 epoch time: 2477.937 ms\n",
      "epoch: 1909 train loss: 0.024726223 epoch time: 2545.249 ms\n",
      "epoch: 1910 train loss: 0.005770206 epoch time: 2496.553 ms\n",
      "epoch: 1911 train loss: 0.008193162 epoch time: 2512.405 ms\n",
      "epoch: 1912 train loss: 0.012596191 epoch time: 2546.973 ms\n",
      "epoch: 1913 train loss: 0.0063599735 epoch time: 2634.492 ms\n",
      "epoch: 1914 train loss: 0.0061288592 epoch time: 2511.891 ms\n",
      "epoch: 1915 train loss: 0.011330539 epoch time: 2484.012 ms\n",
      "epoch: 1916 train loss: 0.014022 epoch time: 2510.683 ms\n",
      "epoch: 1917 train loss: 0.021199996 epoch time: 2535.074 ms\n",
      "epoch: 1918 train loss: 0.024661407 epoch time: 2512.522 ms\n",
      "epoch: 1919 train loss: 0.054760754 epoch time: 2655.057 ms\n",
      "epoch: 1920 train loss: 0.13009995 epoch time: 2976.230 ms\n",
      "epoch: 1921 train loss: 0.352786 epoch time: 2944.452 ms\n",
      "epoch: 1922 train loss: 0.5895181 epoch time: 2895.064 ms\n",
      "epoch: 1923 train loss: 0.15037249 epoch time: 2738.052 ms\n",
      "epoch: 1924 train loss: 0.31109524 epoch time: 2840.518 ms\n",
      "epoch: 1925 train loss: 0.1787264 epoch time: 3067.508 ms\n",
      "epoch: 1926 train loss: 0.13203998 epoch time: 2735.811 ms\n",
      "epoch: 1927 train loss: 0.18605196 epoch time: 2708.352 ms\n",
      "epoch: 1928 train loss: 0.107175164 epoch time: 2514.767 ms\n",
      "epoch: 1929 train loss: 0.14939298 epoch time: 2543.054 ms\n",
      "epoch: 1930 train loss: 0.06467868 epoch time: 2554.458 ms\n",
      "epoch: 1931 train loss: 0.074862644 epoch time: 2500.836 ms\n",
      "epoch: 1932 train loss: 0.068687335 epoch time: 2514.771 ms\n",
      "epoch: 1933 train loss: 0.11207396 epoch time: 2792.124 ms\n",
      "epoch: 1934 train loss: 0.19970626 epoch time: 2517.951 ms\n",
      "epoch: 1935 train loss: 0.34043127 epoch time: 2874.652 ms\n",
      "epoch: 1936 train loss: 0.43253633 epoch time: 2518.669 ms\n",
      "epoch: 1937 train loss: 0.35649395 epoch time: 2800.410 ms\n",
      "epoch: 1938 train loss: 0.17952633 epoch time: 2782.494 ms\n",
      "epoch: 1939 train loss: 0.026115542 epoch time: 2897.080 ms\n",
      "epoch: 1940 train loss: 0.03158752 epoch time: 2705.170 ms\n",
      "epoch: 1941 train loss: 0.05699051 epoch time: 2794.455 ms\n",
      "epoch: 1942 train loss: 0.052922115 epoch time: 2740.191 ms\n",
      "epoch: 1943 train loss: 0.027074685 epoch time: 2851.794 ms\n",
      "epoch: 1944 train loss: 0.009839621 epoch time: 2553.609 ms\n",
      "epoch: 1945 train loss: 0.006345319 epoch time: 2538.393 ms\n",
      "epoch: 1946 train loss: 0.0071651638 epoch time: 2656.880 ms\n",
      "epoch: 1947 train loss: 0.0119763855 epoch time: 2519.393 ms\n",
      "epoch: 1948 train loss: 0.02242279 epoch time: 2838.066 ms\n",
      "epoch: 1949 train loss: 0.04352634 epoch time: 2846.115 ms\n",
      "epoch: 1950 train loss: 0.10057882 epoch time: 2840.073 ms\n",
      "epoch: 1951 train loss: 0.26048905 epoch time: 2550.053 ms\n",
      "epoch: 1952 train loss: 0.70694596 epoch time: 2500.664 ms\n",
      "epoch: 1953 train loss: 1.3293585 epoch time: 2495.944 ms\n",
      "epoch: 1954 train loss: 0.5219966 epoch time: 2576.672 ms\n",
      "epoch: 1955 train loss: 0.1767 epoch time: 2475.310 ms\n",
      "epoch: 1956 train loss: 0.39733598 epoch time: 2585.317 ms\n",
      "epoch: 1957 train loss: 0.046314284 epoch time: 2884.138 ms\n",
      "epoch: 1958 train loss: 0.19049005 epoch time: 2771.205 ms\n",
      "epoch: 1959 train loss: 0.043632556 epoch time: 2724.271 ms\n",
      "epoch: 1960 train loss: 0.065398596 epoch time: 2776.897 ms\n",
      "epoch: 1961 train loss: 0.030843256 epoch time: 2889.979 ms\n",
      "epoch: 1962 train loss: 0.017409204 epoch time: 2879.430 ms\n",
      "epoch: 1963 train loss: 0.025421908 epoch time: 2847.873 ms\n",
      "epoch: 1964 train loss: 0.011405879 epoch time: 2824.809 ms\n",
      "epoch: 1965 train loss: 0.021839067 epoch time: 2805.880 ms\n",
      "epoch: 1966 train loss: 0.027510421 epoch time: 2465.779 ms\n",
      "epoch: 1967 train loss: 0.0310655 epoch time: 2549.467 ms\n",
      "epoch: 1968 train loss: 0.0340788 epoch time: 2639.947 ms\n",
      "epoch: 1969 train loss: 0.02999868 epoch time: 2491.216 ms\n",
      "epoch: 1970 train loss: 0.022524357 epoch time: 2621.897 ms\n",
      "epoch: 1971 train loss: 0.029746171 epoch time: 2499.815 ms\n",
      "epoch: 1972 train loss: 0.06804686 epoch time: 2546.860 ms\n",
      "epoch: 1973 train loss: 0.18587328 epoch time: 2505.929 ms\n",
      "epoch: 1974 train loss: 0.28685218 epoch time: 2532.032 ms\n",
      "epoch: 1975 train loss: 0.22558758 epoch time: 2803.229 ms\n",
      "epoch: 1976 train loss: 0.038377702 epoch time: 2860.494 ms\n",
      "epoch: 1977 train loss: 0.046791278 epoch time: 2727.123 ms\n",
      "epoch: 1978 train loss: 0.09325585 epoch time: 2840.045 ms\n",
      "epoch: 1979 train loss: 0.03608835 epoch time: 2780.082 ms\n",
      "epoch: 1980 train loss: 0.006251455 epoch time: 2971.585 ms\n",
      "epoch: 1981 train loss: 0.012997954 epoch time: 2737.551 ms\n",
      "epoch: 1982 train loss: 0.024311764 epoch time: 2854.480 ms\n",
      "epoch: 1983 train loss: 0.02209564 epoch time: 2700.208 ms\n",
      "epoch: 1984 train loss: 0.032013394 epoch time: 2885.027 ms\n",
      "epoch: 1985 train loss: 0.11530484 epoch time: 2856.257 ms\n",
      "epoch: 1986 train loss: 0.4961954 epoch time: 2875.563 ms\n",
      "epoch: 1987 train loss: 1.3583506 epoch time: 2737.139 ms\n",
      "epoch: 1988 train loss: 1.6928653 epoch time: 2896.847 ms\n",
      "epoch: 1989 train loss: 0.42452124 epoch time: 2725.839 ms\n",
      "epoch: 1990 train loss: 0.73635656 epoch time: 2564.615 ms\n",
      "epoch: 1991 train loss: 0.24241199 epoch time: 2743.640 ms\n",
      "epoch: 1992 train loss: 0.29625145 epoch time: 2980.752 ms\n",
      "epoch: 1993 train loss: 0.22126095 epoch time: 2896.110 ms\n",
      "epoch: 1994 train loss: 0.12181984 epoch time: 2726.385 ms\n",
      "epoch: 1995 train loss: 0.18440096 epoch time: 2787.674 ms\n",
      "epoch: 1996 train loss: 0.015536936 epoch time: 2841.437 ms\n",
      "epoch: 1997 train loss: 0.089802645 epoch time: 2879.203 ms\n",
      "epoch: 1998 train loss: 0.014932893 epoch time: 2813.192 ms\n",
      "epoch: 1999 train loss: 0.03207904 epoch time: 2624.310 ms\n",
      "epoch: 2000 train loss: 0.017408844 epoch time: 2659.294 ms\n",
      "    predict total time: 2.5560855865478516 ms\n",
      "    l2_error:  0.013592685437759764\n",
      "==================================================================================================\n",
      "End-to-End total time: 5426.383932590485 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train()\n",
    "print(\"End-to-End total time: {} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import save_checkpoint\n",
    "\n",
    "save_checkpoint(model, \"darcy2D.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
